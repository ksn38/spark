{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "habrData = spark.read.option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".csv(\"/user/admin/habr_data.csv\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".cache()\n",
    "\n",
    "habrData.printSchema()\n",
    "habrData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(\n",
    "    habrData\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df = habrData\n",
    "dfWindow = Window.partitionBy('author_name').orderBy(col('rating').desc())\n",
    "\n",
    "df2 = df.select('author_name', 'title', 'rating').\\\n",
    "withColumn('row_number', row_number().over(dfWindow)).\\\n",
    "where('row_number < 4')\n",
    "\n",
    "z.show(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По данным habr_data получить топ (по встречаемости) английских слов из заголовков.\n",
    "Возможное решение: 1) выделение слов с помощью регулярных выражений, 2) разделение на массивы слов 3) explode массивовов 4) группировка с подсчетом встречаемости\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, udf, explode, split\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "hd = habrData.select('title').\\\n",
    "withColumn('title2', regexp_replace(col('title'), '[^a-zA-Z\\s]', ' ')).\\\n",
    "withColumn('title3', regexp_replace(col('title2'), '\\s{2,}', ' '))\n",
    "\n",
    "hd = hd.select('title', 'title3').\\\n",
    "withColumn('title4', split(col('title3'), ' '))\n",
    "\n",
    "hd = hd.select(explode(hd.title4).alias('words')).\\\n",
    "na.drop('all').\\\n",
    "groupBy('words').count().orderBy(col('count').desc())\n",
    "\n",
    "hd.printSchema()\n",
    "\n",
    "z.show(hd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "3"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
