{
  "metadata": {
    "name": "practice_habr_data",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType\n\nhabrData \u003d spark.read.option(\"header\", True)\\\n.option(\"inferSchema\", True)\\\n.csv(\"/user/admin/habr_data.csv\")\\\n.withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n.cache()\n\nhabrData.printSchema()\nhabrData.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val habrData \u003d spark.read.option(\"header\", true)\n.option(\"inferSchema\", \"true\")\n.csv(\"/user/admin/habr_data.csv\").cache\n\nhabrData.printSchema\nhabrData.show"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(\n    habrData\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "habrData.select(\"rating\").orderBy(\"rating\").show"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntrainDF, testDF \u003d spark.read.option(\"header\", True)\\\n.csv(\"/user/admin/habr_data.csv\")\\\n.randomSplit([.8, .2], seed\u003d42)\n\ntrainDF.coalesce(2).write.mode(\"overwrite\").saveAsTable(\"habr.train\")\ntestDF.coalesce(2).write.mode(\"overwrite\").saveAsTable(\"habr.test\")\n\nprint(\"There are \" + str(trainDF.count()) + \" rows in the training set, and \" + str(testDF.count()) + \" in the test set\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF, IDF\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\n\n# Prepare training data from a list of (label, features) tuples.\ntrain \u003d spark.table(\"habr.train\")\\\n.selectExpr(\"title\", \"cast(rating as Long) rating\")\\\n.na.drop(\"any\")\n\n# Prepare test data\ntest \u003d spark.table(\"habr.test\")\\\n.selectExpr(\" title\", \"cast(rating as Long) rating\")\\\n.na.drop(\"any\")\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"title\", outputCol\u003d\"title_words\")\n\nregexTokenizer \u003d RegexTokenizer(inputCol\u003d\"title\", outputCol\u003d\"title_words\", pattern\u003d\"[^a-zа-яё]\", gaps\u003dTrue)\\\n.setMinTokenLength(3)\n\n# alternatively, pattern\u003d\"\\\\w+\", gaps(False)\n\n\n# tokenized \u003d tokenizer.transform(train)\n# tokenized.select(\"title\", \"title_words\")\\\n#     .withColumn(\"tokens\", countTokens(col(\"title_words\"))).show(truncate\u003dFalse)\n\nregexTokenized \u003d regexTokenizer.transform(train)\n\n# regexTokenized.select(\"title\", \"title_words\").withColumn(\"tokens\", countTokens(col(\"title_words\"))).show(truncate\u003dFalse)\n    \n\nhashingTF \u003d HashingTF(inputCol\u003d\"title_words\", outputCol\u003d\"rawFeatures\", numFeatures\u003d200000)\nfeaturizedData \u003d hashingTF.transform(regexTokenized)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf \u003d IDF(inputCol\u003d\"rawFeatures\", outputCol\u003d\"features\")\nidfModel \u003d idf.fit(featurizedData)\nrescaledData \u003d idfModel.transform(featurizedData)\n\n# rescaledData.select(\"rating\", \"features\").show()\n\n\nlr \u003d LinearRegression(maxIter\u003d10, regParam\u003d0.1, featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027rating\u0027, predictionCol\u003d\u0027prediction\u0027)\n\np \u003d Pipeline(stages\u003d[])\n\npipeline \u003d Pipeline(stages\u003d[regexTokenizer, hashingTF, idf, lr])\n \n\nmodel \u003d pipeline.fit(train)\nprediction \u003d model.transform(test)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nregressionEvaluator \u003d RegressionEvaluator(\n    predictionCol\u003d\"prediction\",\n    labelCol\u003d\"rating\",\n    metricName\u003d\"rmse\")\n    \nrmse \u003d regressionEvaluator.evaluate(prediction)\nprint(\"RMSE is \" + str(rmse))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntokenizer.transform(train)\\\n.show(20, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nRegexTokenizer(inputCol\u003d\"title\", outputCol\u003d\"title_words\", pattern\u003d\"[^a-zа-яё]\", gaps\u003dTrue)\\\n.setMinTokenLength(3).transform(train)\\\n.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfeaturizedData.select(\"title_words\", \"rawFeatures\" ).show(20, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nidf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrescaledData.select(\"features\").show(10, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nhashingTF \u003d HashingTF(inputCol\u003d\"title_words\", outputCol\u003d\"rawFeatures\", numFeatures\u003d100000)\nfeaturizedData \u003d hashingTF.transform(regexTokenized)\n\nfeaturizedData.select(\"title\", \"rawFeatures\").show(100, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "Seq(\"Reverse Reverse Reverse\").toDF(\"title\").write.mode(\"overwrite\").saveAsTable(\"habr.df\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf \u003d spark.table(\"habr.df\")\ndf.show()\n\nhashingTF.transform(regexTokenizer.transform(df)).show(1, False)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nregexTokenizer \u003d RegexTokenizer(inputCol\u003d\"title\", outputCol\u003d\"title_words\", pattern\u003d\"[^a-zа-яё]\", gaps\u003dTrue)\\\n.setMinTokenLength(3)\n\nregexTokenizer.transform(train).show(100, False)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\nparamGrid \u003d ParamGridBuilder()  \\\n    .addGrid(lr.regParam, [0.01, 0.1])\\\n    .addGrid(hashingTF.numFeatures, [1000, 2000]) \\\n    .build()\n    # .addGrid(lr.regParam, [0.1, 0.01]) \\\n\ncrossval \u003d CrossValidator(estimator\u003dpipeline,\n                          estimatorParamMaps\u003dparamGrid,\n                          evaluator\u003dRegressionEvaluator(),\n                          numFolds\u003d3)  # use 3+ folds in practice\n\n# Run cross-validation, and choose the best set of parameters.\n\ncvModel \u003d crossval.fit(train.withColumn(\"label\", col(\"rating\")))\n\n\n# tvs \u003d TrainValidationSplit(estimator\u003dpipeline,\n#                           estimatorParamMaps\u003dparamGrid,\n#                           evaluator\u003dregressionEvaluator,\n#                           trainRatio\u003d0.8)\n\n# tvsModel \u003d tvs.fit(train)\n\n# predictionTvs \u003d tvsModel.transform(test)\n\nprediction \u003d cvModel.transform(test)\n\nrmse \u003d regressionEvaluator.evaluate(prediction)\nprint(\"RMSE is \" + str(rmse))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.sql.functions import col\n\n# Prepare training and test data.\ndata \u003d rescaledData.withColumn(\"label\", col(\"rating\"))\ntrain, test \u003d data.randomSplit([0.9, 0.1], seed\u003d12345)\n\nlr \u003d LinearRegression(maxIter\u003d10)\n\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using\n# the evaluator.\nparamGrid \u003d ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 1.0])\\\n    .build()\n\n# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs \u003d TrainValidationSplit(estimator\u003dlr,\n                           estimatorParamMaps\u003dparamGrid,\n                           evaluator\u003dRegressionEvaluator(),\n                           # 80% of the data will be used for training, 20% for validation.\n                           trainRatio\u003d0.8)\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel \u003d tvs.fit(train)\n\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\nmodel.bestModel.transform(test)\\\n    .select(\"features\", \"label\", \"prediction\")\\\n    .show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nmodel.bestModel.coefficients"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* построить распределение статей в датасете по rating с bin_size \u003d 10\n* написать функцию ratingToClass(rating: Int): String, которая определяет категорию статьи ( A, B, C, D) на основе рейтинга. Границы для классов подобрать самостоятельно.\n* добавить к датасету категориальную фичу \"rating_class\".  При добавлении колонки использовать udf из функции в предыдущем пункте\n* Построить модель логистической регрессии (one vs all)  для классификации статей по рассчитанным классам.\n* Получить F1 score для получившейся модели"
    }
  ]
}