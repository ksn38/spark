{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Zeppelin.\n",
    "##### This is a live tutorial, you can run the code yourself. (Shift-Enter to Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заголовок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "du -d 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -du -s -h /apps/spark/warehouse/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sc.stop\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "z.show(\n",
    "    spark.sql(\"show databases\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "print(\"Application ID: \" + sc.applicationId + \"\\n\")\n",
    "\n",
    "print(\"Spark configuration: \\n\")\n",
    "\n",
    "for pair in spark.sparkContext.getConf().getAll():\n",
    "    print(pair[0] + \" \\t= \" + pair[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark \n",
    "\n",
    "dept = [(\"Finance\", 10), \n",
    "        (\"Marketing\", 20), \n",
    "        (\"Sales\", 30), \n",
    "        (\"IT\", 40) \n",
    "      ]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "\n",
    "deptDF.printSchema()\n",
    "\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "z.show(deptDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "nmn_dataset_path = \"/user/admin/mnm_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "bank_schema = \"State STRING, Color STRING, Count INTEGER\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(bank_schema)\\\n",
    "    .csv(nmn_dataset_path)\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df.write.mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"default.mnm\")\n",
    "\n",
    "\n",
    "spark.table(\"default.mnm\").printSchema()\n",
    "\n",
    "spark.table(\"default.mnm\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.table(\"default.mnm\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.sql(\"show tables in default\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls /apps/spark/warehouse/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls /apps/spark/warehouse/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"homework.bank\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(\n",
    "    spark.sql(\"select * from mnm\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df \\\n",
    ".repartition(10) \\\n",
    ".write.mode(\"overwrite\")\\\n",
    ".saveAsTable(\"default.mnm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "\n",
    "hdfs dfs -ls /apps/spark/warehouse/mnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"default.mnm\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"default.mnm\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "import scala.io.Source._\n",
    "import org.apache.spark.sql.{Dataset, SparkSession}\n",
    "\n",
    "val url = \"https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\"\n",
    "var res = fromURL(url).mkString.stripMargin.lines.toList\n",
    "val csvData: Dataset[String] = spark.sparkContext.parallelize(res).toDS()\n",
    "\n",
    "val bank = spark.read.option(\"header\", true).option(\"delimiter\", \";\").option(\"inferSchema\",true).csv(csvData)\n",
    "bank.printSchema()\n",
    "\n",
    "bank.write.mode(\"overwrite\").saveAsTable(\"homework.bank\")\n",
    "\n",
    "\n",
    "spark.sql(\"refresh table homework.bank\")\n",
    "spark.table(\"homework.bank\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"homework.bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "print(\"Hello \" + z.textbox(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.read.csv(\"/user/admin/sf-fire-calls.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# create DataFrame from python list. It can infer schema for you.\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "    \n",
    "SPARK_HOME = os.getenv('SPARK_HOME')\n",
    "\n",
    "# Read data from json file\n",
    "# link for this people.json (https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json)\n",
    "# Use hdfs path if you are using hdfs\n",
    "df1 = spark.read.json(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.json\")\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "# Read data from csv file. You can customize it via spark.read.options. E.g. In the following example, we customize the sep and header\n",
    "df2 = spark.read.options(sep=\";\", header=True).csv(\"file://\"  + SPARK_HOME + \"/examples/src/main/resources/people.csv\")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# Specify schema for your csv file\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "schema = StructType().add(\"name\", StringType(), True) \\\n",
    "    .add(\"age\", IntegerType(), True) \\\n",
    "    .add(\"job\", StringType(), True)\n",
    "    \n",
    "df3 = spark.read.options(sep=\";\", header=True) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.csv\") \n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# withColumn could be used to add new Column\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "df2 = df1.withColumn(\"age2\", df1[\"age\"] + 1)\n",
    "df2.show()\n",
    "\n",
    "# the new column could replace the existing the column if the new column name is the same as the old column\n",
    "df3 = df1.withColumn(\"age\", df1[\"age\"] + 1)\n",
    "df3.show()\n",
    "\n",
    "# Besides using expression to create new column, you could also use udf to create new column\n",
    "# Use F.upper instead of upper, because the builtin udf of spark may conclifct with that of python, such as max\n",
    "import pyspark.sql.functions as F\n",
    "df4 = df1.withColumn(\"name\", F.upper(df1[\"name\"]))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "# drop could be used to remove Column\n",
    "df2 = df1.drop(\"id\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"homework.bank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "# select can accept a list of string of the column names\n",
    "df2 = df1.select(\"id\", \"name\")\n",
    "df2.show()\n",
    "\n",
    "# select can also accept a list of Column. You can create column via $ or udf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df3 = df1.select(df1[\"id\"], F.upper(df1[\"name\"]), df1[\"age\"] + 1)\n",
    "df3.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "# filter accept a Column \n",
    "df2 = df1.filter(df1[\"age\"] >= 20)\n",
    "df2.show()\n",
    "\n",
    "# To be noticed, you need to use \"&\" instead of \"&&\" or \"AND\" \n",
    "df3 = df1.filter((df1[\"age\"] >= 20) & (df1[\"country\"] == \"China\"))\n",
    "df3.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n",
    "            .toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "# Create udf create python lambda\n",
    "from pyspark.sql.functions import udf\n",
    "udf1 = udf(lambda e: e.upper())\n",
    "df2 = df1.select(udf1(df1[\"name\"]))\n",
    "df2.show()\n",
    "\n",
    "# UDF could also be used in filter, in this case the return type must be Boolean\n",
    "# We can also use annotation to create udf\n",
    "from pyspark.sql.types import *\n",
    "@udf(returnType=BooleanType())\n",
    "def udf2(e):\n",
    "    if e >= 20:\n",
    "        return True;\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df3 = df1.filter(udf2(df1[\"age\"]))\n",
    "df3.show()\n",
    "\n",
    "# UDF could also accept more than 1 argument.\n",
    "udf3 = udf(lambda e1, e2: e1 + \"_\" + e2)\n",
    "df4 = df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n",
    "           .toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "# You can call agg function after groupBy directly, such as count/min/max/avg/sum\n",
    "df2 = df1.groupBy(\"country\").count()\n",
    "df2.show()\n",
    "\n",
    "# Pass a Map if you want to do multiple aggregation\n",
    "df3 = df1.groupBy(\"country\").agg({\"age\": \"avg\", \"id\": \"count\"})\n",
    "df3.show()\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "# Or you can pass a list of agg function\n",
    "df4 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.count(df1[\"id\"]).alias(\"count\"))\n",
    "df4.show()\n",
    "\n",
    "# You can not pass Map if you want to do multiple aggregation on the same column as the key of Map should be unique. So in this case\n",
    "# you have to pass a list of agg functions\n",
    "df5 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.max(df1[\"age\"]).alias(\"max_age\"))\n",
    "df5.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, 1), (2, \"jeff\", 23, 2), (3, \"james\", 18, 3)]).toDF(\"id\", \"name\", \"age\", \"c_id\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\n",
    "df2.show()\n",
    "\n",
    "# You can just specify the key name if join on the same key\n",
    "df3 = df1.join(df2, \"c_id\")\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case the key is different between tables\n",
    "df4 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"])\n",
    "df4.show()\n",
    "\n",
    "# You can specify the join type afte the join condition, by default it is inner join\n",
    "df5 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"], \"left_outer\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\n",
    "df2.show()\n",
    "\n",
    "# Join on 2 fields: key_1, key_2\n",
    "\n",
    "# You can pass a list of field name if the join field names are the same in both tables\n",
    "df3 = df1.join(df2, [\"key_1\", \"key_2\"])\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\n",
    "df4 = df1.join(df2, (df1[\"key_1\"] == df2[\"key_1\"]) & (df1[\"key_2\"] == df2[\"key_2\"]))\n",
    "df4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n",
    "           .toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "# call createOrReplaceTempView first if you want to query this DataFrame via sql\n",
    "df1.createOrReplaceTempView(\"people\")\n",
    "# SparkSession.sql return DataFrame\n",
    "df2 = spark.sql(\"select name, age from people\")\n",
    "df2.show()\n",
    "\n",
    "# You need to register udf if you want to use it in sql\n",
    "spark.udf.register(\"udf1\", lambda e : e.upper())\n",
    "df3 = spark.sql(\"select udf1(name), age from people\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "    [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "    [\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Read the file into a Spark DataFrame using the CSV\n",
    "# format by inferring the schema and specifying that the\n",
    "# file contains a header, which provides column names for comma-\n",
    "# separated fields.\n",
    "mnm_df = (spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(nmn_dataset_path))\n",
    "\n",
    "# We use the DataFrame high-level APIs. Note\n",
    "# that we don't use RDDs at all. Because some of Spark's\n",
    "# functions return the same object, we can chain function calls.\n",
    "# 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "# 2. Since we want to group each state and its M&M color count,\n",
    "# we use groupBy()\n",
    "# 3. Aggregate counts of all colors and groupBy() State and Color\n",
    "# 4 orderBy() in descending order\n",
    "count_mnm_df = (mnm_df\n",
    "    .select(\"State\", \"Color\", \"Count\")\n",
    "    .groupBy(\"State\", \"Color\")\n",
    "    .agg(count(\"Count\").alias(\"Total\"))\n",
    "    .orderBy(\"Total\", ascending=False))\n",
    "    \n",
    "    \n",
    "# Show the resulting aggregations for all the states and colors;\n",
    "# a total count of each color per state.\n",
    "# Note show() is an action, which will trigger the above\n",
    "# query to be executed.\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "lecture 1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
