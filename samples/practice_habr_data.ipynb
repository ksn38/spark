{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "habrData = spark.read.option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".csv(\"/user/admin/habr_data.csv\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".cache()\n",
    "\n",
    "habrData.printSchema()\n",
    "habrData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val habrData = spark.read.option(\"header\", true)\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"/user/admin/habr_data.csv\").cache\n",
    "\n",
    "habrData.printSchema\n",
    "habrData.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(\n",
    "    habrData\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "habrData.select(\"rating\").orderBy(\"rating\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "trainDF, testDF = spark.read.option(\"header\", True)\\\n",
    ".csv(\"/user/admin/habr_data.csv\")\\\n",
    ".randomSplit([.8, .2], seed=42)\n",
    "\n",
    "trainDF.coalesce(2).write.mode(\"overwrite\").saveAsTable(\"habr.train\")\n",
    "testDF.coalesce(2).write.mode(\"overwrite\").saveAsTable(\"habr.test\")\n",
    "\n",
    "print(\"There are \" + str(trainDF.count()) + \" rows in the training set, and \" + str(testDF.count()) + \" in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF, IDF\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "train = spark.table(\"habr.train\")\\\n",
    ".selectExpr(\"title\", \"cast(rating as Long) rating\")\\\n",
    ".na.drop(\"any\")\n",
    "\n",
    "# Prepare test data\n",
    "test = spark.table(\"habr.test\")\\\n",
    ".selectExpr(\" title\", \"cast(rating as Long) rating\")\\\n",
    ".na.drop(\"any\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_words\", pattern=\"[^a-zа-яё]\", gaps=True)\\\n",
    ".setMinTokenLength(3)\n",
    "\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "\n",
    "# tokenized = tokenizer.transform(train)\n",
    "# tokenized.select(\"title\", \"title_words\")\\\n",
    "#     .withColumn(\"tokens\", countTokens(col(\"title_words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(train)\n",
    "\n",
    "# regexTokenized.select(\"title\", \"title_words\").withColumn(\"tokens\", countTokens(col(\"title_words\"))).show(truncate=False)\n",
    "    \n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"title_words\", outputCol=\"rawFeatures\", numFeatures=200000)\n",
    "featurizedData = hashingTF.transform(regexTokenized)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# rescaledData.select(\"rating\", \"features\").show()\n",
    "\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.1, featuresCol='features', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "p = Pipeline(stages=[])\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, hashingTF, idf, lr])\n",
    " \n",
    "\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"rating\",\n",
    "    metricName=\"rmse\")\n",
    "    \n",
    "rmse = regressionEvaluator.evaluate(prediction)\n",
    "print(\"RMSE is \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "tokenizer.transform(train)\\\n",
    ".show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "RegexTokenizer(inputCol=\"title\", outputCol=\"title_words\", pattern=\"[^a-zа-яё]\", gaps=True)\\\n",
    ".setMinTokenLength(3).transform(train)\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "featurizedData.select(\"title_words\", \"rawFeatures\" ).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rescaledData.select(\"features\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"title_words\", outputCol=\"rawFeatures\", numFeatures=100000)\n",
    "featurizedData = hashingTF.transform(regexTokenized)\n",
    "\n",
    "featurizedData.select(\"title\", \"rawFeatures\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "Seq(\"Reverse Reverse Reverse\").toDF(\"title\").write.mode(\"overwrite\").saveAsTable(\"habr.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df = spark.table(\"habr.df\")\n",
    "df.show()\n",
    "\n",
    "hashingTF.transform(regexTokenizer.transform(df)).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_words\", pattern=\"[^a-zа-яё]\", gaps=True)\\\n",
    ".setMinTokenLength(3)\n",
    "\n",
    "regexTokenizer.transform(train).show(100, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "paramGrid = ParamGridBuilder()  \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1])\\\n",
    "    .addGrid(hashingTF.numFeatures, [1000, 2000]) \\\n",
    "    .build()\n",
    "    # .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "\n",
    "cvModel = crossval.fit(train.withColumn(\"label\", col(\"rating\")))\n",
    "\n",
    "\n",
    "# tvs = TrainValidationSplit(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=regressionEvaluator,\n",
    "#                           trainRatio=0.8)\n",
    "\n",
    "# tvsModel = tvs.fit(train)\n",
    "\n",
    "# predictionTvs = tvsModel.transform(test)\n",
    "\n",
    "prediction = cvModel.transform(test)\n",
    "\n",
    "rmse = regressionEvaluator.evaluate(prediction)\n",
    "print(\"RMSE is \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = rescaledData.withColumn(\"label\", col(\"rating\"))\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.bestModel.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* построить распределение статей в датасете по rating с bin_size = 10\n",
    "* написать функцию ratingToClass(rating: Int): String, которая определяет категорию статьи ( A, B, C, D) на основе рейтинга. Границы для классов подобрать самостоятельно.\n",
    "* добавить к датасету категориальную фичу \"rating_class\".  При добавлении колонки использовать udf из функции в предыдущем пункте\n",
    "* Построить модель логистической регрессии (one vs all)  для классификации статей по рассчитанным классам.\n",
    "* Получить F1 score для получившейся модели"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "practice_habr_data"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
