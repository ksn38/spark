{"paragraphs":[{"text":"%md\n## Welcome to Zeppelin.\n##### This is a live tutorial, you can run the code yourself. (Shift-Enter to Run)","user":"admin","dateUpdated":"2020-12-18T18:30:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Welcome to Zeppelin.</h2>\n<h5>This is a live tutorial, you can run the code yourself. (Shift-Enter to Run)</h5>\n"}]},"apps":[],"jobName":"paragraph_1605774626288_-1766496140","id":"20201119-083026_765367917","dateCreated":"2020-11-19T08:30:26+0000","dateStarted":"2020-12-18T18:30:50+0000","dateFinished":"2020-12-18T18:30:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7465"},{"text":"\nsc.applicationId\n","user":"admin","dateUpdated":"2021-01-11T18:49:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res2: String = application_1610373458327_0001\n"}]},"apps":[],"jobName":"paragraph_1608316894903_965579296","id":"20201218-184134_285919498","dateCreated":"2020-12-18T18:41:34+0000","dateStarted":"2021-01-11T18:21:47+0000","dateFinished":"2021-01-11T18:21:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7466"},{"text":"spark.sparkContext.applicationId","user":"admin","dateUpdated":"2021-01-11T18:23:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: String = application_1610373458327_0004\n"}]},"apps":[],"jobName":"paragraph_1610389376303_-223929297","id":"20210111-182256_119124479","dateCreated":"2021-01-11T18:22:56+0000","dateStarted":"2021-01-11T18:23:53+0000","dateFinished":"2021-01-11T18:24:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7467"},{"text":"%pyspark\n\nz.show(\n    spark.sql(\"show databases\")\n)\n","user":"admin","dateUpdated":"2020-12-18T19:00:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"databaseName","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"databaseName":"number"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"databaseName","index":0,"aggr":"sum"}],"groups":[],"values":[]},"helium":{}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"databaseName\nbd_228_aafonin\nbd_228_akurov\nbd_228_ddoni\nbd_274_mmingalov\nbd_274_rozhkov\ndefault\nfc\nhabr\nhomework\nhw2\nkotelnikov\noberezovskaya\nsint_sales\n"},{"type":"TEXT","data":"\n"}]},"apps":[],"jobName":"paragraph_1608310041661_-992747101","id":"20201218-164721_486785588","dateCreated":"2020-12-18T16:47:21+0000","dateStarted":"2020-12-18T18:32:58+0000","dateFinished":"2020-12-18T18:32:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7468"},{"text":"val a = \" 123\"\nprintln(a)","user":"admin","dateUpdated":"2020-12-18T18:36:08+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":" 123\na: String = \" 123\"\n"}]},"apps":[],"jobName":"paragraph_1608316539283_1130160958","id":"20201218-183539_1906218396","dateCreated":"2020-12-18T18:35:39+0000","dateStarted":"2020-12-18T18:35:55+0000","dateFinished":"2020-12-18T18:35:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7469"},{"text":"%pyspark\n\nnmn_dataset_path = \"/user/admin/mnm_dataset.csv\"","user":"admin","dateUpdated":"2020-12-18T19:02:49+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1605774656264_-1322789819","id":"20201119-083056_1408855290","dateCreated":"2020-11-19T08:30:56+0000","dateStarted":"2020-12-18T19:02:49+0000","dateFinished":"2020-12-18T19:02:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7470"},{"text":"%pyspark\nspark.stop()","user":"admin","dateUpdated":"2020-12-18T18:40:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1608316815911_1904760411","id":"20201218-184015_1920995165","dateCreated":"2020-12-18T18:40:15+0000","dateStarted":"2020-12-18T18:40:24+0000","dateFinished":"2020-12-18T18:40:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7471"},{"text":"%pyspark\nspark.sql(\"test\")","user":"admin","dateUpdated":"2020-12-18T18:40:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-5115799069085235266.py\", line 363, in <module>\n    sc.setJobGroup(jobGroup, jobDesc)\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/context.py\", line 971, in setJobGroup\n    self._jsc.setJobGroup(groupId, description, interruptOnCancel)\nAttributeError: 'NoneType' object has no attribute 'setJobGroup'\n"}]},"apps":[],"jobName":"paragraph_1608316831116_-209797718","id":"20201218-184031_1112699696","dateCreated":"2020-12-18T18:40:31+0000","dateStarted":"2020-12-18T18:40:58+0000","dateFinished":"2020-12-18T18:40:58+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:7472"},{"text":"%pyspark\n","user":"admin","dateUpdated":"2020-12-18T18:40:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1608316815691_1291295884","id":"20201218-184015_291628108","dateCreated":"2020-12-18T18:40:15+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7473"},{"title":"Spark configuration parameters","text":"%pyspark\n\nprint(\"Application ID: \" + sc.applicationId + \"\\n\")\n\nprint(\"Spark configuration: \\n\")\n\nfor pair in spark.sparkContext.getConf().getAll():\n    print(pair[0] + \" \\t= \" + pair[1]) ","user":"admin","dateUpdated":"2021-01-11T18:27:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Application ID: application_1610373458327_0004\n\nSpark configuration: \n\nspark.history.kerberos.keytab \t= none\nspark.eventLog.enabled \t= true\nmaster \t= yarn-client\nspark.driver.host \t= bigdataanalytics-head-0.novalocal\nspark.driver.extraJavaOptions \t=  -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///usr/hdp/current/zeppelin-server/conf/log4j.properties -Dzeppelin.log.file=/var/log/zeppelin/zeppelin-interpreter-spark2-zeppelin-bigdataanalytics-head-0.novalocal.log\nspark.repl.class.outputDir \t= /tmp/spark1984835311016345805\nzeppelin.spark.concurrentSQL \t= false\nzeppelin.pyspark.useIPython \t= true\nspark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES \t= http://bigdataanalytics-head-0.novalocal:8088/proxy/application_1610373458327_0004\nspark.history.ui.port \t= 18081\nspark.driver.extraLibraryPath \t= /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64\nspark.history.fs.cleaner.interval \t= 7d\nzeppelin.spark.useNew \t= true\nspark.shuffle.io.serverThreads \t= 128\nspark.executor.extraLibraryPath \t= /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64\nSPARK_HOME \t= /usr/hdp/current/spark2-client/\nspark.executorEnv.PYTHONPATH \t= /usr/hdp/current/spark2-client//python/lib/py4j-0.10.7-src.zip:/usr/hdp/current/spark2-client//python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\nzeppelin.interpreter.localRepo \t= /usr/hdp/current/zeppelin-server/local-repo/spark2\nspark.driver.appUIAddress \t= http://bigdataanalytics-head-0.novalocal:4040\nspark.shuffle.file.buffer \t= 1m\nspark.useHiveContext \t= true\nspark.sql.hive.convertMetastoreOrc \t= true\nspark.sql.autoBroadcastJoinThreshold \t= 26214400\nspark.ui.filters \t= org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\nspark.eventLog.dir \t= hdfs:///spark2-history/\nspark.yarn.historyServer.address \t= bigdataanalytics-head-0.novalocal:18081\nspark.repl.class.uri \t= spark://bigdataanalytics-head-0.novalocal:40933/classes\nspark.ui.proxyBase \t= /proxy/application_1610373458327_0001\nzeppelin.spark.printREPLOutput \t= true\nzeppelin.spark.enableSupportedVersionCheck \t= true\nspark.executor.memory \t= 2G\nspark.executor.id \t= driver\nspark.app.id \t= application_1610373458327_0004\nspark.sql.orc.impl \t= native\nspark.yarn.dist.archives \t= file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr\nzeppelin.R.image.width \t= 100%\nspark.history.fs.logDirectory \t= hdfs:///spark2-history/\nspark.sql.catalogImplementation \t= hive\nspark.history.fs.cleaner.maxAge \t= 90d\nspark.cores.max \t= 2\nzeppelin.spark.sql.interpolation \t= false\nspark.driver.port \t= 40933\nspark.sql.warehouse.dir \t= /apps/spark/warehouse\nspark.history.store.path \t= /var/lib/spark2/shs_db\nspark.jars \t= file:/usr/hdp/current/zeppelin-server/interpreter/spark/spark-interpreter-0.8.0.3.1.4.0-315.jar\nspark.dynamicAllocation.maxExecutors \t= 4\nzeppelin.R.cmd \t= R\nzeppelin.dep.localrepo \t= local-repo\nzeppelin.spark.importImplicit \t= true\nzeppelin.spark.maxResult \t= 50000\nspark.sql.statistics.fallBackToHdfs \t= true\nzeppelin.R.render.options \t= out.format = 'html', comment = NA, echo = FALSE, results = 'asis', message = F, warning = F\nspark.history.provider \t= org.apache.spark.deploy.history.FsHistoryProvider\nspark.submit.deployMode \t= client\nzeppelin.R.knitr \t= true\nspark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS \t= bigdataanalytics-head-0.novalocal\nzeppelin.spark.useHiveContext \t= true\nspark.sql.hive.metastore.jars \t= /usr/hdp/current/spark2-client/standalone-metastore/*\nspark.driver.extraClassPath \t= /etc/zeppelin/conf/external-dependency-conf:/usr/hdp/current/zeppelin-server/interpreter/spark/*:/usr/hdp/current/zeppelin-server/lib/interpreter/*::/usr/hdp/current/zeppelin-server/interpreter/spark/spark-interpreter-0.8.0.3.1.4.0-315.jar:/etc/hadoop/conf\nzeppelin.pyspark.python \t= python\nspark.yarn.queue \t= default\nspark.history.fs.cleaner.enabled \t= true\nzeppelin.spark.sql.stacktrace \t= false\nspark.master \t= yarn\nspark.io.compression.lz4.blockSize \t= 128kb\nspark.history.kerberos.principal \t= none\nspark.sql.orc.filterPushdown \t= true\nspark.shuffle.io.backLog \t= 8192\nzeppelin.interpreter.output.limit \t= 1024000\nspark.dynamicAllocation.executorIdleTimeout \t= 600s\nspark.app.name \t= Zeppelin\nspark.unsafe.sorter.spill.reader.buffer.size \t= 1m\nspark.shuffle.unsafe.file.output.buffer \t= 5m\nzeppelin.dep.additionalRemoteRepository \t= spark-packages,http://dl.bintray.com/spark-packages/maven,false;\nspark.yarn.isPython \t= true\nzeppelin.interpreter.max.poolsize \t= 10\nspark.executor.extraJavaOptions \t= -XX:+UseNUMA\nspark.sql.hive.metastore.version \t= 3.0\n"}]},"apps":[],"jobName":"paragraph_1605776964881_-1076785018","id":"20201119-090924_1760414506","dateCreated":"2020-11-19T09:09:24+0000","dateStarted":"2021-01-11T18:27:11+0000","dateFinished":"2021-01-11T18:27:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7474"},{"title":"Create DataFrame from array","text":"%pyspark \n\ndept = [(\"Finance\",10), \n        (\"Marketing\",20), \n        (\"Sales\",30), \n        (\"IT\",40) \n      ]\n\ndeptColumns = [\"dept_name\",\"dept_id\"]\n\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n\ndeptDF.printSchema()\n\ndeptDF.show(truncate=False)","user":"admin","dateUpdated":"2021-01-11T18:30:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605776964573_-1557888232","id":"20201119-090924_666128713","dateCreated":"2020-11-19T09:09:24+0000","dateStarted":"2021-01-11T18:30:31+0000","dateFinished":"2021-01-11T18:30:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7475"},{"text":"%pyspark\nz.show(deptDF)","user":"admin","dateUpdated":"2021-01-15T09:34:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"dept_name","visible":true,"width":"*","sort":{"priority":0,"direction":"asc"},"filters":[{}],"pinned":""},{"name":"dept_id","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"dept_name":"string","dept_id":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"dept_name","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"dept_id","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dept_name\tdept_id\nFinance\t10\nMarketing\t20\nSales\t30\nIT\t40\n"},{"type":"TEXT","data":"\n"}]},"apps":[],"jobName":"paragraph_1610389834859_-1110078175","id":"20210111-183034_250552806","dateCreated":"2021-01-11T18:30:34+0000","dateStarted":"2021-01-11T18:30:52+0000","dateFinished":"2021-01-11T18:30:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7476"},{"title":".csv to DataFrame","text":"%pyspark\n\nbank_schema = \"State STRING, Color STRING, Count INTEGER\"\n\ndf = spark.read \\\n    .option(\"header\", True) \\\n    .schema(bank_schema)\\\n    .csv(nmn_dataset_path)\n\nprint(df.count())\n\ndf.printSchema()","user":"admin","dateUpdated":"2021-01-11T18:31:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"99999\nroot\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1605776964401_-730744409","id":"20201119-090924_1728313471","dateCreated":"2020-11-19T09:09:24+0000","dateStarted":"2020-12-18T19:09:53+0000","dateFinished":"2020-12-18T19:09:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7477"},{"title":"Save to table","text":"%pyspark\n\ndf.write.mode(\"overwrite\")\\\n    .saveAsTable(\"default.mnm\")\n\n\nspark.table(\"default.mnm\").printSchema()\n\nspark.table(\"default.mnm\").rdd.getNumPartitions()","user":"admin","dateUpdated":"2021-01-11T18:31:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: string (nullable = true)\n\n1\n"}]},"apps":[],"jobName":"paragraph_1605777090543_1146167000","id":"20201119-091130_1783946172","dateCreated":"2020-11-19T09:11:30+0000","dateStarted":"2020-12-18T19:07:23+0000","dateFinished":"2020-12-18T19:07:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7478"},{"text":"%pyspark\nspark.sql(\"show databases\").show()","user":"admin","dateUpdated":"2020-12-17T16:20:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+\n|    databaseName|\n+----------------+\n|  bd_228_aafonin|\n|   bd_228_akurov|\n|    bd_228_ddoni|\n|bd_274_mmingalov|\n|  bd_274_rozhkov|\n|         default|\n|              fc|\n|            habr|\n|        homework|\n|             hw2|\n|      kotelnikov|\n|   oberezovskaya|\n|      sint_sales|\n+----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1605809514271_353774126","id":"20201119-181154_149916491","dateCreated":"2020-11-19T18:11:54+0000","dateStarted":"2020-12-17T16:20:41+0000","dateFinished":"2020-12-17T16:20:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7479"},{"text":"%pyspark\nspark.sql(\"show tables in homework\").show()","user":"admin","dateUpdated":"2020-12-17T16:20:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n|homework|     bank|      false|\n+--------+---------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1605809549615_1583302697","id":"20201119-181229_1852689705","dateCreated":"2020-11-19T18:12:29+0000","dateStarted":"2020-12-17T16:20:41+0000","dateFinished":"2020-12-17T16:20:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7480"},{"text":"%pyspark\n\nspark.read.parquet(\"/apps/spark/warehouse/homework.db/bank/part-00000-de52cfa9-2904-46fe-85b8-cb223c03fd5b-c000.snappy.parquet\")\\\n.show()","user":"admin","dateUpdated":"2020-12-17T16:20:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 1: spark.read.parquet(\"/apps/spark/warehouse/homework.db/bank/part-00000-de52cfa9-2904-46fe-85b8-cb223c03fd5b-c000.snappy.parquet\")\\\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9104713557996593786.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 303, in parquet\n    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\nAnalysisException: u'Path does not exist: hdfs://bigdataanalytics-head-0.novalocal:8020/apps/spark/warehouse/homework.db/bank/part-00000-de52cfa9-2904-46fe-85b8-cb223c03fd5b-c000.snappy.parquet;'\n"}]},"apps":[],"jobName":"paragraph_1605809684106_-2090425683","id":"20201119-181444_1560852028","dateCreated":"2020-11-19T18:14:44+0000","dateStarted":"2020-12-17T16:20:42+0000","dateFinished":"2020-12-17T16:20:42+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:7481"},{"title":"Партиции на диске","text":"%sh\n\nhdfs dfs -ls /apps/spark/warehouse/mnm","user":"BD_256_ivoznenko","dateUpdated":"2020-11-25T05:29:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 2 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-11-25 04:49 /apps/spark/warehouse/mnm/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs     176920 2020-11-25 04:49 /apps/spark/warehouse/mnm/part-00000-be120ca3-a3d6-4a7c-9833-2f6e06ab15c8-c000.snappy.parquet\n"}]},"apps":[],"jobName":"paragraph_1605777090267_1681014018","id":"20201119-091130_392962640","dateCreated":"2020-11-19T09:11:30+0000","dateStarted":"2020-11-25T05:29:38+0000","dateFinished":"2020-11-25T05:29:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7482"},{"text":"spark.table(\"homework.bank\").show","user":"BD_274_aivanov","dateUpdated":"2020-11-23T14:15:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n| 30|   unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n| 35|   management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n| 30|   management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n| 59|  blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n| 35|   management| single| tertiary|     no|    747|     no|  no|cellular| 23|  feb|     141|       2|  176|       3| failure| no|\n| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n| 39|   technician|married|secondary|     no|    147|    yes|  no|cellular|  6|  may|     151|       2|   -1|       0| unknown| no|\n| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n| 43|     services|married|  primary|     no|    -88|    yes| yes|cellular| 17|  apr|     313|       1|  147|       2| failure| no|\n| 39|     services|married|secondary|     no|   9374|    yes|  no| unknown| 20|  may|     273|       1|   -1|       0| unknown| no|\n| 43|       admin.|married|secondary|     no|    264|    yes|  no|cellular| 17|  apr|     113|       2|   -1|       0| unknown| no|\n| 36|   technician|married| tertiary|     no|   1109|     no|  no|cellular| 13|  aug|     328|       2|   -1|       0| unknown| no|\n| 20|      student| single|secondary|     no|    502|     no|  no|cellular| 30|  apr|     261|       1|   -1|       0| unknown|yes|\n| 31|  blue-collar|married|secondary|     no|    360|    yes| yes|cellular| 29|  jan|      89|       1|  241|       1| failure| no|\n| 40|   management|married| tertiary|     no|    194|     no| yes|cellular| 29|  aug|     189|       2|   -1|       0| unknown| no|\n| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n| 25|  blue-collar| single|  primary|     no|   -221|    yes|  no| unknown| 23|  may|     250|       1|   -1|       0| unknown| no|\n| 31|     services|married|secondary|     no|    132|     no|  no|cellular|  7|  jul|     148|       1|  152|       1|   other| no|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1605811914157_-1210425638","id":"20201119-185154_464618414","dateCreated":"2020-11-19T18:51:54+0000","dateStarted":"2020-11-23T14:15:51+0000","dateFinished":"2020-11-23T14:15:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7483"},{"text":"%pyspark\n\ndeptDF.createOrReplaceTempView(\"test\")","user":"admin","dateUpdated":"2021-01-11T18:37:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1610390196123_-1779898098","id":"20210111-183636_1182374721","dateCreated":"2021-01-11T18:36:36+0000","dateStarted":"2021-01-11T18:37:39+0000","dateFinished":"2021-01-11T18:37:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7484"},{"text":"%sql\n\nselect * from test","user":"admin","dateUpdated":"2021-01-11T18:48:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"State":"string","Color":"string","Count":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"State","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Color","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 6 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:837)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:835)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:835)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1890)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1803)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1936)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1361)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1935)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.close(BaseSparkScalaInterpreter.scala:152)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.close(SparkScala211Interpreter.scala:100)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.close(NewSparkInterpreter.java:132)\n\tat org.apache.zeppelin.spark.SparkInterpreter.close(SparkInterpreter.java:67)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.close(LazyOpenInterpreter.java:85)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer.close(RemoteInterpreterServer.java:410)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$close.getResult(RemoteInterpreterService.java:1827)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$close.getResult(RemoteInterpreterService.java:1807)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkZeppelinContext.showData(SparkZeppelinContext.java:108)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:135)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:633)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1605811503230_-1137968281","id":"20201119-184503_1883561312","dateCreated":"2020-11-19T18:45:03+0000","dateStarted":"2021-01-11T18:37:44+0000","dateFinished":"2021-01-11T18:46:36+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:7485"},{"text":"z.show(\n    spark.sql(\"select * from mnm\")\n    )","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"State","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Color","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Count","visible":true,"width":"*","sort":{"priority":0,"direction":"desc"},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"State":"string","Color":"string","Count":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"State","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Count","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"State\tColor\tCount\nTX\tRed\t20\nNV\tBlue\t66\nCO\tBlue\t79\nOR\tBlue\t71\nWA\tYellow\t93\nWY\tBlue\t16\nCA\tYellow\t53\nWA\tGreen\t60\nOR\tGreen\t71\nTX\tGreen\t68\nNV\tGreen\t59\nAZ\tBrown\t95\nWA\tYellow\t20\nAZ\tBlue\t75\nOR\tBrown\t72\nNV\tRed\t98\nWY\tOrange\t45\nCO\tBlue\t52\nTX\tBrown\t94\nCO\tRed\t82\nCO\tRed\t12\nCO\tRed\t17\nOR\tGreen\t16\nAZ\tGreen\t46\nNV\tRed\t43\nNM\tYellow\t15\nWA\tRed\t12\nOR\tGreen\t13\nCO\tBlue\t95\nWY\tRed\t63\nTX\tOrange\t63\nWY\tYellow\t48\nOR\tGreen\t95\nWA\tRed\t75\nCO\tOrange\t93\nNV\tOrange\t10\nWY\tGreen\t15\nWA\tGreen\t99\nCO\tBlue\t98\nCA\tGreen\t86\nUT\tRed\t92\nAZ\tBrown\t16\nCA\tRed\t100\nUT\tRed\t77\nTX\tYellow\t29\nWA\tOrange\t73\nWY\tYellow\t56\nWA\tYellow\t19\nWY\tYellow\t18\nAZ\tOrange\t49\nWA\tRed\t83\nAZ\tRed\t59\nCA\tBlue\t13\nCO\tBrown\t53\nCA\tRed\t23\nCA\tYellow\t49\nNV\tBlue\t50\nCO\tRed\t81\nUT\tGreen\t62\nNM\tGreen\t38\nNV\tYellow\t77\nTX\tBlue\t49\nWA\tBrown\t20\nOR\tGreen\t59\nOR\tRed\t66\nCA\tBlue\t34\nUT\tBlue\t97\nAZ\tBlue\t59\nOR\tBlue\t29\nCO\tRed\t17\nAZ\tOrange\t54\nTX\tGreen\t69\nNM\tBrown\t65\nWY\tOrange\t23\nNM\tGreen\t42\nOR\tGreen\t39\nWA\tOrange\t84\nUT\tYellow\t72\nNV\tYellow\t98\nTX\tYellow\t20\nUT\tBrown\t58\nCA\tRed\t67\nCO\tOrange\t42\nWA\tRed\t70\nOR\tBlue\t54\nTX\tGreen\t85\nWY\tBrown\t85\nUT\tGreen\t97\nCO\tBlue\t52\nWA\tYellow\t43\nWA\tYellow\t77\nWA\tGreen\t10\nTX\tRed\t97\nWA\tBrown\t21\nWY\tRed\t22\nWY\tYellow\t42\nWA\tOrange\t30\nNV\tOrange\t48\nWA\tBlue\t59\nCO\tOrange\t22\nCA\tOrange\t25\nCA\tYellow\t69\nWY\tBlue\t59\nUT\tBrown\t77\nTX\tGreen\t82\nCA\tOrange\t91\nTX\tBrown\t96\nWY\tBlue\t53\nWA\tBlue\t42\nTX\tGreen\t74\nAZ\tBrown\t79\nOR\tOrange\t85\nCO\tOrange\t21\nAZ\tYellow\t48\nWA\tBlue\t37\nNV\tBlue\t49\nCA\tBrown\t95\nCO\tBrown\t53\nTX\tRed\t16\nCO\tBrown\t56\nNV\tRed\t90\nWY\tBrown\t87\nUT\tYellow\t56\nNM\tYellow\t21\nCA\tYellow\t41\nCA\tOrange\t71\nUT\tYellow\t97\nCA\tBrown\t58\nWY\tBrown\t43\nNM\tBlue\t15\nAZ\tYellow\t72\nWA\tBrown\t38\nCO\tGreen\t44\nNV\tBrown\t65\nAZ\tYellow\t94\nNV\tRed\t94\nCA\tBrown\t92\nWA\tOrange\t94\nAZ\tOrange\t53\nWY\tGreen\t18\nNM\tOrange\t69\nCA\tBlue\t39\nNM\tGreen\t32\nWA\tGreen\t68\nAZ\tGreen\t77\nUT\tRed\t46\nNM\tGreen\t87\nCO\tRed\t78\nUT\tBlue\t48\nNV\tBrown\t10\nNM\tOrange\t77\nUT\tGreen\t60\nWA\tYellow\t49\nOR\tOrange\t18\nOR\tYellow\t72\nCO\tOrange\t41\nAZ\tBlue\t82\nUT\tBrown\t78\nUT\tOrange\t24\nUT\tGreen\t97\nWA\tBrown\t26\nCO\tBrown\t98\nCO\tBlue\t14\nWY\tYellow\t78\nUT\tOrange\t34\nWA\tBlue\t18\nCO\tBlue\t89\nTX\tBlue\t80\nUT\tYellow\t81\nNM\tYellow\t75\nWY\tGreen\t10\nCO\tRed\t79\nUT\tBrown\t98\nWA\tBlue\t84\nNM\tBlue\t46\nAZ\tBlue\t16\nWA\tBrown\t17\nTX\tBlue\t68\nNM\tGreen\t67\nAZ\tGreen\t89\nOR\tBrown\t48\nWY\tGreen\t91\nWA\tBrown\t52\nAZ\tGreen\t29\nWY\tBrown\t30\nNM\tRed\t50\nNV\tBrown\t49\nCA\tBlue\t99\nWY\tOrange\t81\nUT\tGreen\t41\nOR\tBrown\t35\nTX\tRed\t13\nWY\tBlue\t80\nOR\tGreen\t76\nCO\tYellow\t74\nUT\tGreen\t33\nTX\tBrown\t55\nCO\tGreen\t22\nWY\tOrange\t99\nTX\tOrange\t93\nTX\tYellow\t83\nOR\tGreen\t77\nCO\tYellow\t37\nCO\tBlue\t44\nCA\tBrown\t80\nWY\tBrown\t36\nWA\tBrown\t26\nNV\tGreen\t49\nAZ\tRed\t93\nTX\tYellow\t19\nCO\tYellow\t63\nTX\tYellow\t81\nTX\tRed\t100\nCA\tRed\t45\nUT\tBrown\t59\nNM\tBrown\t66\nTX\tRed\t95\nOR\tYellow\t28\nAZ\tYellow\t83\nCO\tRed\t52\nNV\tRed\t23\nWA\tGreen\t27\nWY\tYellow\t90\nNM\tYellow\t29\nCA\tGreen\t73\nNV\tBrown\t34\nCA\tOrange\t32\nTX\tRed\t44\nWA\tBrown\t76\nCO\tBlue\t90\nCO\tRed\t82\nNV\tOrange\t58\nNM\tRed\t40\nTX\tBlue\t19\nNV\tOrange\t84\nNM\tGreen\t61\nUT\tBrown\t92\nAZ\tOrange\t50\nAZ\tYellow\t49\nCA\tBlue\t65\nNM\tRed\t50\nWA\tYellow\t57\nTX\tYellow\t64\nCO\tRed\t94\nAZ\tYellow\t80\nTX\tBlue\t98\nOR\tBrown\t48\nNM\tBlue\t98\nAZ\tYellow\t80\nOR\tOrange\t91\nCA\tBrown\t64\nOR\tGreen\t39\nNV\tYellow\t76\nNM\tRed\t53\nUT\tYellow\t35\nAZ\tBlue\t24\nWY\tRed\t87\nWY\tGreen\t57\nNM\tRed\t47\nCO\tBrown\t57\nWY\tGreen\t97\nWA\tYellow\t31\nWY\tBrown\t81\nCA\tRed\t27\nWA\tYellow\t58\nNM\tOrange\t70\nWY\tGreen\t39\nWA\tRed\t38\nNM\tBrown\t99\nCA\tBrown\t94\nCO\tYellow\t14\nCO\tGreen\t25\nNV\tRed\t52\nWY\tBrown\t48\nNV\tOrange\t65\nAZ\tYellow\t41\nWY\tGreen\t68\nUT\tGreen\t23\nWA\tGreen\t58\nNM\tOrange\t41\nWY\tOrange\t56\nWY\tRed\t70\nCO\tGreen\t58\nUT\tOrange\t36\nCA\tRed\t50\nWA\tRed\t10\nCA\tYellow\t42\nAZ\tGreen\t99\nNM\tBrown\t36\nUT\tGreen\t77\nTX\tGreen\t91\nWA\tOrange\t91\nOR\tBrown\t16\nCA\tOrange\t87\nTX\tYellow\t70\nWA\tGreen\t29\nNM\tBlue\t70\nTX\tRed\t55\nWY\tGreen\t86\nOR\tBlue\t96\nOR\tBlue\t37\nWY\tBlue\t33\nNM\tGreen\t83\nUT\tOrange\t87\nWY\tYellow\t62\nWY\tRed\t23\nTX\tBrown\t59\nCO\tGreen\t40\nCA\tBrown\t41\nAZ\tBrown\t90\nWA\tYellow\t92\nOR\tGreen\t76\nWA\tBlue\t46\nCA\tRed\t52\nNM\tBlue\t52\nWY\tOrange\t63\nOR\tGreen\t14\nAZ\tYellow\t23\nCA\tBrown\t84\nNM\tOrange\t46\nWY\tGreen\t100\nCA\tRed\t49\nUT\tBrown\t67\nUT\tBrown\t52\nAZ\tBrown\t41\nOR\tBlue\t20\nCA\tYellow\t94\nCO\tBlue\t27\nCA\tYellow\t89\nNM\tBlue\t82\nUT\tYellow\t26\nAZ\tGreen\t35\nUT\tGreen\t72\nCA\tYellow\t66\nWY\tYellow\t58\nNV\tYellow\t96\nWY\tGreen\t36\nWA\tBrown\t35\nCA\tGreen\t39\nCA\tRed\t71\nOR\tBrown\t93\nOR\tBlue\t74\nUT\tGreen\t42\nNM\tYellow\t34\nNM\tRed\t22\nWY\tGreen\t48\nWY\tYellow\t59\nCA\tBlue\t43\nUT\tBlue\t21\nWA\tBrown\t96\nWA\tBlue\t62\nCO\tRed\t22\nNM\tBlue\t70\nTX\tGreen\t83\nOR\tBrown\t65\nNM\tRed\t34\nTX\tBrown\t12\nUT\tGreen\t86\nUT\tYellow\t28\nOR\tRed\t53\nTX\tBrown\t37\nWA\tBlue\t37\nUT\tRed\t35\nCO\tOrange\t72\nAZ\tBlue\t19\nUT\tBlue\t94\nOR\tBrown\t61\nUT\tOrange\t77\nTX\tRed\t45\nNV\tBrown\t72\nNM\tBlue\t81\nAZ\tYellow\t39\nOR\tYellow\t28\nWA\tGreen\t40\nUT\tOrange\t60\nTX\tBlue\t41\nTX\tGreen\t93\nCA\tOrange\t72\nCO\tOrange\t99\nNV\tBlue\t89\nTX\tBlue\t76\nAZ\tGreen\t79\nTX\tBlue\t59\nNM\tGreen\t20\nCA\tYellow\t32\nOR\tBlue\t95\nNM\tOrange\t96\nNM\tRed\t57\nUT\tBlue\t65\nWY\tBlue\t35\nWY\tBlue\t24\nUT\tOrange\t58\nTX\tOrange\t77\nWA\tBlue\t14\nCO\tBlue\t27\nCA\tBlue\t23\nWY\tBrown\t20\nCO\tBlue\t17\nWA\tYellow\t14\nCA\tGreen\t61\nCO\tBrown\t90\nAZ\tYellow\t76\nWY\tRed\t85\nCO\tBlue\t22\nUT\tBrown\t50\nWY\tOrange\t75\nOR\tYellow\t25\nOR\tGreen\t76\nCA\tBlue\t99\nCO\tGreen\t27\nNV\tOrange\t45\nOR\tRed\t80\nUT\tYellow\t92\nWA\tBrown\t16\nAZ\tBrown\t39\nNM\tBlue\t13\nCO\tYellow\t17\nUT\tOrange\t55\nOR\tRed\t49\nAZ\tGreen\t36\nAZ\tYellow\t37\nCA\tRed\t34\nWY\tBlue\t57\nTX\tBlue\t60\nUT\tYellow\t99\nWY\tGreen\t23\nCA\tGreen\t53\nAZ\tBrown\t43\nTX\tOrange\t18\nCO\tGreen\t54\nNV\tRed\t92\nTX\tBlue\t91\nAZ\tOrange\t77\nWA\tBrown\t58\nTX\tOrange\t73\nWY\tBlue\t62\nCA\tBlue\t89\nAZ\tGreen\t92\nWY\tBrown\t67\nAZ\tYellow\t14\nUT\tBrown\t56\nTX\tOrange\t28\nUT\tBrown\t19\nWA\tBlue\t42\nNM\tYellow\t30\nWY\tOrange\t81\nCO\tBrown\t72\nNM\tBlue\t21\nCA\tBlue\t65\nAZ\tBlue\t38\nAZ\tOrange\t39\nAZ\tBrown\t42\nNV\tOrange\t21\nCO\tBrown\t26\nWY\tOrange\t15\nCO\tRed\t67\nUT\tYellow\t34\nAZ\tYellow\t71\nNV\tBlue\t12\nAZ\tGreen\t25\nNM\tRed\t65\nOR\tOrange\t20\nNM\tRed\t14\nCO\tOrange\t79\nTX\tOrange\t99\nAZ\tGreen\t58\nWA\tBrown\t12\nUT\tBrown\t60\nCO\tRed\t93\nUT\tBrown\t72\nAZ\tOrange\t88\nCA\tYellow\t34\nWY\tYellow\t32\nNM\tGreen\t99\nOR\tBlue\t89\nTX\tYellow\t60\nTX\tGreen\t45\nOR\tYellow\t40\nWA\tRed\t66\nCO\tYellow\t43\nWA\tBlue\t47\nWA\tGreen\t83\nWA\tOrange\t37\nWY\tYellow\t36\nUT\tBlue\t19\nNM\tOrange\t11\nTX\tYellow\t15\nWA\tOrange\t41\nCO\tRed\t90\nTX\tYellow\t52\nNV\tRed\t72\nNM\tYellow\t52\nWA\tGreen\t37\nOR\tYellow\t82\nCO\tGreen\t31\nNV\tYellow\t99\nWA\tBrown\t91\nAZ\tOrange\t50\nCA\tBlue\t42\nTX\tYellow\t15\nWY\tBlue\t65\nNM\tGreen\t64\nCO\tOrange\t29\nWA\tOrange\t61\nUT\tRed\t83\nCO\tBlue\t73\nWA\tBlue\t42\nWA\tBrown\t59\nOR\tBrown\t79\nCO\tGreen\t26\nOR\tRed\t92\nWY\tGreen\t23\nNV\tRed\t39\nCA\tOrange\t70\nUT\tOrange\t14\nCA\tOrange\t10\nNM\tGreen\t82\nWA\tBlue\t15\nTX\tRed\t91\nUT\tRed\t88\nWY\tRed\t10\nWA\tYellow\t74\nWA\tGreen\t96\nCO\tBlue\t47\nNV\tBlue\t13\nUT\tBlue\t52\nCO\tOrange\t65\nOR\tOrange\t33\nCO\tYellow\t77\nCO\tOrange\t19\nOR\tBrown\t89\nCA\tRed\t83\nNM\tBrown\t75\nNV\tOrange\t38\nWY\tGreen\t95\nAZ\tGreen\t54\nNM\tOrange\t22\nTX\tOrange\t82\nCO\tYellow\t72\nTX\tYellow\t67\nUT\tBlue\t19\nOR\tRed\t55\nAZ\tBrown\t69\nTX\tBlue\t26\nCA\tBrown\t34\nCA\tOrange\t48\nAZ\tOrange\t64\nWY\tGreen\t62\nNV\tBrown\t41\nWA\tGreen\t31\nCA\tRed\t57\nWA\tBrown\t53\nCA\tOrange\t71\nNV\tBrown\t69\nTX\tOrange\t25\nNV\tRed\t59\nTX\tOrange\t86\nWA\tYellow\t70\nUT\tGreen\t16\nUT\tBlue\t87\nUT\tBrown\t23\nTX\tGreen\t98\nCA\tGreen\t28\nTX\tBlue\t86\nWY\tGreen\t68\nNV\tBrown\t35\nCO\tBlue\t13\nCO\tOrange\t89\nCA\tOrange\t66\nWA\tYellow\t84\nOR\tGreen\t64\nOR\tRed\t45\nUT\tGreen\t65\nWY\tOrange\t21\nTX\tBrown\t18\nUT\tOrange\t59\nUT\tGreen\t49\nNV\tOrange\t98\nCA\tGreen\t48\nCA\tYellow\t25\nCO\tRed\t44\nUT\tYellow\t85\nNM\tBrown\t30\nNM\tBrown\t89\nOR\tBrown\t40\nNM\tYellow\t58\nTX\tYellow\t31\nOR\tOrange\t33\nNM\tGreen\t24\nNM\tOrange\t70\nAZ\tBrown\t93\nCA\tOrange\t96\nNV\tBrown\t36\nWY\tYellow\t49\nWA\tYellow\t34\nTX\tRed\t64\nTX\tBlue\t99\nCO\tGreen\t34\nCO\tBlue\t12\nNM\tYellow\t10\nNM\tOrange\t76\nNM\tRed\t57\nCA\tRed\t61\nCO\tYellow\t82\nUT\tBlue\t99\nWY\tRed\t80\nWA\tOrange\t51\nTX\tOrange\t86\nCO\tGreen\t20\nWA\tBrown\t10\nWA\tYellow\t40\nAZ\tOrange\t22\nNM\tGreen\t61\nCA\tBlue\t27\nNM\tBlue\t23\nUT\tGreen\t35\nUT\tOrange\t59\nTX\tYellow\t32\nWA\tYellow\t78\nAZ\tBrown\t35\nWA\tBlue\t22\nOR\tOrange\t63\nWA\tOrange\t92\nWY\tGreen\t78\nCA\tOrange\t35\nAZ\tRed\t92\nCA\tRed\t14\nWA\tRed\t52\nUT\tBlue\t22\nCA\tYellow\t57\nAZ\tYellow\t30\nCA\tGreen\t77\nWY\tYellow\t67\nNV\tYellow\t95\nAZ\tBrown\t26\nAZ\tRed\t58\nWY\tBrown\t14\nTX\tOrange\t52\nCO\tOrange\t95\nNV\tGreen\t91\nOR\tBrown\t86\nWY\tRed\t19\nTX\tBrown\t78\nWY\tOrange\t87\nCA\tRed\t91\nNV\tBrown\t50\nCO\tBrown\t28\nAZ\tOrange\t50\nUT\tBrown\t41\nCO\tYellow\t66\nUT\tBrown\t76\nOR\tRed\t30\nNM\tBrown\t30\nUT\tOrange\t21\nWY\tBlue\t48\nAZ\tGreen\t49\nTX\tBrown\t32\nNV\tBlue\t71\nNV\tYellow\t83\nOR\tYellow\t85\nAZ\tGreen\t35\nWY\tRed\t81\nWY\tOrange\t76\nUT\tGreen\t42\nWA\tRed\t17\nAZ\tBrown\t81\nTX\tBlue\t90\nAZ\tRed\t57\nWA\tGreen\t32\nNM\tYellow\t66\nUT\tOrange\t43\nTX\tYellow\t14\nWA\tOrange\t61\nNV\tYellow\t62\nWA\tGreen\t16\nOR\tYellow\t33\nTX\tOrange\t66\nUT\tOrange\t40\nUT\tRed\t100\nTX\tOrange\t28\nNV\tOrange\t26\nWA\tRed\t33\nAZ\tBlue\t85\nNV\tRed\t100\nNV\tBrown\t64\nWA\tYellow\t71\nNV\tGreen\t94\nWA\tBrown\t46\nUT\tYellow\t27\nOR\tGreen\t90\nWA\tYellow\t41\nCA\tGreen\t38\nWA\tOrange\t93\nOR\tYellow\t82\nWY\tRed\t16\nWA\tBrown\t32\nWY\tBlue\t38\nTX\tYellow\t96\nWY\tBlue\t58\nWA\tGreen\t37\nWA\tYellow\t35\nNV\tYellow\t66\nOR\tBlue\t31\nAZ\tGreen\t15\nNV\tBlue\t34\nWY\tGreen\t24\nNM\tBrown\t16\nNV\tOrange\t50\nWY\tBlue\t28\nNM\tBlue\t29\nTX\tYellow\t37\nCO\tBrown\t30\nOR\tOrange\t91\nTX\tYellow\t35\nNM\tOrange\t20\nCA\tBlue\t19\nCO\tRed\t64\nCA\tBrown\t72\nTX\tBrown\t67\nWA\tGreen\t64\nWY\tBlue\t24\nOR\tBrown\t84\nWA\tBrown\t78\nAZ\tGreen\t61\nTX\tYellow\t51\nWY\tYellow\t23\nCO\tRed\t87\nOR\tBlue\t78\nUT\tBrown\t99\nCO\tRed\t24\nNM\tRed\t43\nNM\tGreen\t66\nUT\tYellow\t82\nWA\tRed\t28\nWY\tBrown\t28\nAZ\tRed\t65\nNV\tYellow\t58\nUT\tBrown\t84\nCO\tYellow\t11\nNM\tYellow\t29\nOR\tBlue\t17\nWA\tYellow\t56\nCO\tYellow\t43\nCO\tOrange\t43\nUT\tOrange\t32\nOR\tBrown\t86\nOR\tGreen\t94\nTX\tGreen\t94\nNV\tOrange\t54\nAZ\tRed\t99\nTX\tRed\t54\nCA\tGreen\t57\nUT\tBrown\t88\nAZ\tRed\t59\nUT\tRed\t93\nCA\tGreen\t61\nNV\tYellow\t60\nOR\tRed\t98\nWY\tRed\t37\nWA\tBlue\t42\nWY\tYellow\t89\nCA\tGreen\t98\nTX\tOrange\t36\nUT\tBrown\t68\nWA\tRed\t93\nNV\tOrange\t84\nNV\tGreen\t93\nOR\tYellow\t52\nOR\tRed\t87\nUT\tGreen\t50\nNM\tRed\t64\nTX\tBlue\t59\nAZ\tBrown\t81\nWA\tBrown\t96\nCO\tOrange\t80\nNM\tBrown\t25\nNV\tBlue\t57\nCA\tBlue\t88\nAZ\tBlue\t69\nUT\tYellow\t54\nUT\tRed\t80\nCA\tYellow\t92\nNM\tBlue\t36\nWA\tGreen\t54\nWA\tBlue\t67\nAZ\tBrown\t39\nNV\tGreen\t32\nNV\tGreen\t27\nUT\tYellow\t93\nCA\tOrange\t18\nWA\tYellow\t37\nTX\tBlue\t11\nCA\tYellow\t69\nCO\tOrange\t43\nOR\tBrown\t88\nNM\tBrown\t24\nCA\tRed\t39\nCO\tYellow\t16\nWY\tOrange\t49\nOR\tRed\t73\nUT\tBlue\t38\nTX\tGreen\t87\nAZ\tOrange\t89\nNM\tGreen\t88\nWY\tBrown\t25\nWY\tRed\t24\nNV\tYellow\t61\nWY\tBlue\t43\nUT\tOrange\t22\nWA\tBrown\t97\nUT\tRed\t57\nWY\tRed\t95\nAZ\tBlue\t63\nWY\tRed\t17\nCO\tBrown\t99\nOR\tOrange\t43\nCO\tBlue\t51\nTX\tBlue\t41\nNM\tRed\t15\nWA\tBlue\t16\nCO\tGreen\t46\nWA\tRed\t13\nCA\tBlue\t81\nNM\tGreen\t84\nAZ\tBlue\t88\nUT\tYellow\t100\nUT\tYellow\t57\nWY\tBrown\t34\nAZ\tBrown\t25\nWA\tOrange\t59\nOR\tOrange\t74\nCO\tOrange\t24\nCO\tYellow\t69\nCO\tBlue\t43\nTX\tGreen\t36\nNV\tBlue\t62\nNV\tBrown\t29\nAZ\tYellow\t98\nWA\tBrown\t10\nWY\tRed\t36\nNV\tGreen\t36\nCA\tRed\t12\nCO\tBlue\t44\nUT\tOrange\t89\nNV\tRed\t24\nWA\tYellow\t95\nNM\tBrown\t86\nOR\tBrown\t29\nTX\tOrange\t41\nWA\tOrange\t14\nCO\tBrown\t87\nCO\tOrange\t96\nWA\tYellow\t91\nAZ\tGreen\t24\nUT\tGreen\t13\nNM\tGreen\t13\nNM\tBrown\t58\nOR\tOrange\t18\nWA\tYellow\t83\nUT\tBrown\t82\nWA\tBrown\t59\nWY\tBlue\t58\nTX\tYellow\t98\nWY\tOrange\t56\nTX\tYellow\t79\nCO\tBlue\t99\nCO\tBrown\t62\nWA\tOrange\t92\nCA\tBrown\t16\nUT\tBlue\t34\nNM\tRed\t88\nNM\tYellow\t95\nNM\tBlue\t76\nWY\tRed\t46\nWA\tRed\t89\nWY\tBrown\t78\nTX\tYellow\t59\nCO\tYellow\t21\nCO\tBrown\t85\nCO\tOrange\t15\nAZ\tOrange\t85\nAZ\tYellow\t85\nNM\tGreen\t68\nWY\tBlue\t50\nUT\tYellow\t93\nTX\tBrown\t88\nNV\tBrown\t10\nUT\tYellow\t37\nNM\tGreen\t54\nNV\tRed\t68\nAZ\tOrange\t61\nNM\tYellow\t73\nNV\tRed\t98\nTX\tRed\t98\nCA\tOrange\t47\nNV\tRed\t18\nWY\tBrown\t15\nTX\tYellow\t89\nTX\tBrown\t27\nTX\tRed\t10\nNM\tYellow\t63\nWA\tGreen\t70\nTX\tYellow\t86\nWA\tBrown\t55\nNM\tYellow\t48\nCA\tRed\t67\nTX\tOrange\t35\nAZ\tBlue\t85\nAZ\tGreen\t44\nWA\tGreen\t99\nWY\tBlue\t90\nUT\tBrown\t66\nTX\tBrown\t69\nTX\tOrange\t47\nAZ\tRed\t75\nOR\tBlue\t59\nWA\tOrange\t55\nWA\tOrange\t93\nWA\tGreen\t20\nWY\tBrown\t76\nCO\tYellow\t33\nNM\tBlue\t95\nTX\tGreen\t59\nWA\tBrown\t21\nAZ\tYellow\t19\nUT\tBrown\t18\nUT\tBrown\t82\nNV\tBlue\t97\nNV\tYellow\t96\nNV\tGreen\t41\nUT\tRed\t16\nCA\tYellow\t71\nNV\tBlue\t18\nTX\tGreen\t16\nCA\tRed\t22\nCA\tBrown\t96\nCO\tBlue\t26\nCA\tRed\t28\nTX\tBrown\t94\nAZ\tGreen\t54\nTX\tGreen\t14\nNM\tRed\t58\nWA\tRed\t49\nAZ\tYellow\t83\nNV\tBlue\t58\nTX\tBlue\t94\nWY\tYellow\t100\nTX\tBrown\t39\nUT\tGreen\t19\nNV\tOrange\t17\nNM\tBrown\t59\nCO\tGreen\t86\nAZ\tBrown\t68\nAZ\tBrown\t29\nNM\tRed\t57\nCO\tBrown\t15\nAZ\tOrange\t45\nWA\tBlue\t96\nOR\tBrown\t48\nUT\tBrown\t59\nWY\tOrange\t84\nWA\tYellow\t45\nWY\tGreen\t52\nUT\tRed\t90\nCO\tRed\t22\nTX\tBrown\t92\nWA\tYellow\t92\nUT\tYellow\t53\nUT\tOrange\t82\nUT\tRed\t91\nNM\tGreen\t27\nNV\tGreen\t96\nWA\tRed\t32\nCA\tBrown\t90\nWY\tBlue\t18\nNV\tOrange\t86\nNV\tRed\t34\nNV\tBrown\t30\nWY\tGreen\t57\nUT\tRed\t84\nNM\tGreen\t45\nUT\tYellow\t31\nUT\tYellow\t80\nOR\tRed\t79\nUT\tBrown\t15\nNV\tOrange\t51\nCO\tBrown\t73\nUT\tYellow\t67\nWA\tBrown\t88\nCA\tYellow\t60\nTX\tOrange\t34\nWA\tRed\t92\nWA\tBrown\t68\nNV\tOrange\t18\nWY\tBrown\t60\nNV\tRed\t52\nTX\tGreen\t45\nWA\tRed\t82\nWA\tGreen\t29\nUT\tRed\t34\n"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 1000 rows. Learn more about <strong>zeppelin.spark.maxResult</strong></div>\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1605810997871_67990378","id":"20201119-183637_1720828841","dateCreated":"2020-11-19T18:36:37+0000","dateStarted":"2020-11-22T14:27:33+0000","dateFinished":"2020-11-22T14:27:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7486"},{"user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605811069839_1140835843","id":"20201119-183749_1760572595","dateCreated":"2020-11-19T18:37:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7487"},{"title":"Перезаписываем на большее число партиций","text":"%pyspark\n\ndf \\\n.repartition(10) \\\n.write.mode(\"overwrite\")\\\n.saveAsTable(\"default.mnm\")","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1605777090120_-1093574212","id":"20201119-091130_207597223","dateCreated":"2020-11-19T09:11:30+0000","dateStarted":"2020-11-22T14:27:33+0000","dateFinished":"2020-11-22T14:27:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7488"},{"title":"Проверяем","text":"%sh\n\n\nhdfs dfs -ls /apps/spark/warehouse/mnm","user":"BD_256_glarin","dateUpdated":"2021-01-11T18:41:57+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 11 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-11-22 14:27 /apps/spark/warehouse/mnm/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs      18869 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00000-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00001-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00002-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00003-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00004-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00005-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18865 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00006-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18867 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00007-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18868 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00008-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n-rw-r--r--   3 zeppelin hdfs      18868 2020-11-22 14:27 /apps/spark/warehouse/mnm/part-00009-620d4275-8aaf-46d1-9d9c-508d3d3d9c1f-c000.snappy.parquet\n"}]},"apps":[],"jobName":"paragraph_1605777089951_-2053921658","id":"20201119-091129_809152569","dateCreated":"2020-11-19T09:11:29+0000","dateStarted":"2020-11-22T14:27:36+0000","dateFinished":"2020-11-22T14:27:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7489"},{"title":"Читаем и смотрим количество tasks","text":"%pyspark\n\nspark.table(\"default.mnm\").count()","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"99999\n"}]},"apps":[],"jobName":"paragraph_1605777515234_1581130810","id":"20201119-091835_291608152","dateCreated":"2020-11-19T09:18:35+0000","dateStarted":"2020-11-22T14:27:38+0000","dateFinished":"2020-11-22T14:27:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7490"},{"text":"%pyspark\n\nspark.table(\"default.mnm\").rdd.getNumPartitions()","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2\n"}]},"apps":[],"jobName":"paragraph_1605777515081_426559489","id":"20201119-091835_353155606","dateCreated":"2020-11-19T09:18:35+0000","dateStarted":"2020-11-22T14:27:38+0000","dateFinished":"2020-11-22T14:27:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7491"},{"title":"Сохранение таблицы для ДЗ","text":"\nimport scala.io.Source._\nimport org.apache.spark.sql.{Dataset, SparkSession}\n\nval url = \"https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\"\nvar res = fromURL(url).mkString.stripMargin.lines.toList\nval csvData: Dataset[String] = spark.sparkContext.parallelize(res).toDS()\n\nval bank = spark.read.option(\"header\", true).option(\"delimiter\", \";\").option(\"inferSchema\",true).csv(csvData)\nbank.printSchema()\n\nbank.write.mode(\"overwrite\").saveAsTable(\"homework.bank\")\n\n\nspark.sql(\"refresh table homework.bank\")\nspark.table(\"homework.bank\")\n.show","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: integer (nullable = true)\n |-- job: string (nullable = true)\n |-- marital: string (nullable = true)\n |-- education: string (nullable = true)\n |-- default: string (nullable = true)\n |-- balance: integer (nullable = true)\n |-- housing: string (nullable = true)\n |-- loan: string (nullable = true)\n |-- contact: string (nullable = true)\n |-- day: integer (nullable = true)\n |-- month: string (nullable = true)\n |-- duration: integer (nullable = true)\n |-- campaign: integer (nullable = true)\n |-- pdays: integer (nullable = true)\n |-- previous: integer (nullable = true)\n |-- poutcome: string (nullable = true)\n |-- y: string (nullable = true)\n\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n| 30|   unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n| 35|   management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n| 30|   management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n| 59|  blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n| 35|   management| single| tertiary|     no|    747|     no|  no|cellular| 23|  feb|     141|       2|  176|       3| failure| no|\n| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n| 39|   technician|married|secondary|     no|    147|    yes|  no|cellular|  6|  may|     151|       2|   -1|       0| unknown| no|\n| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n| 43|     services|married|  primary|     no|    -88|    yes| yes|cellular| 17|  apr|     313|       1|  147|       2| failure| no|\n| 39|     services|married|secondary|     no|   9374|    yes|  no| unknown| 20|  may|     273|       1|   -1|       0| unknown| no|\n| 43|       admin.|married|secondary|     no|    264|    yes|  no|cellular| 17|  apr|     113|       2|   -1|       0| unknown| no|\n| 36|   technician|married| tertiary|     no|   1109|     no|  no|cellular| 13|  aug|     328|       2|   -1|       0| unknown| no|\n| 20|      student| single|secondary|     no|    502|     no|  no|cellular| 30|  apr|     261|       1|   -1|       0| unknown|yes|\n| 31|  blue-collar|married|secondary|     no|    360|    yes| yes|cellular| 29|  jan|      89|       1|  241|       1| failure| no|\n| 40|   management|married| tertiary|     no|    194|     no| yes|cellular| 29|  aug|     189|       2|   -1|       0| unknown| no|\n| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n| 25|  blue-collar| single|  primary|     no|   -221|    yes|  no| unknown| 23|  may|     250|       1|   -1|       0| unknown| no|\n| 31|     services|married|secondary|     no|    132|     no|  no|cellular|  7|  jul|     148|       1|  152|       1|   other| no|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\nonly showing top 20 rows\n\nimport scala.io.Source._\nimport org.apache.spark.sql.{Dataset, SparkSession}\nurl: String = https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\nres: List[String] = List(\"age\";\"job\";\"marital\";\"education\";\"default\";\"balance\";\"housing\";\"loan\";\"contact\";\"day\";\"month\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"y\", 30;\"unemployed\";\"married\";\"primary\";\"no\";1787;\"no\";\"no\";\"cellular\";19;\"oct\";79;1;-1;0;\"unknown\";\"no\", 33;\"services\";\"married\";\"secondary\";\"no\";4789;\"yes\";\"yes\";\"cellular\";11;\"may\";220;1;339;4;\"failure\";\"no\", 35;\"management\";\"single\";\"tertiary\";\"no\";1350;\"yes\";\"no\";\"cellular\";16;\"apr\";185;1;330;1;\"failure\";\"no\", 30;\"management\";\"married\";\"tertiary\";\"no\";1476;\"yes\";\"yes\";\"unknown\";3;\"jun\";199;4;-1;0;\"unknown\";\"no\", 59;\"blue-collar\";\"married\";\"secondary\";\"no\";0;\"y..."}]},"apps":[],"jobName":"paragraph_1605777514924_-1512678234","id":"20201119-091834_845082435","dateCreated":"2020-11-19T09:18:34+0000","dateStarted":"2020-11-19T09:33:38+0000","dateFinished":"2020-11-19T09:33:41+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7492"},{"text":"spark.table(\"homework.bank\")","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res3: org.apache.spark.sql.DataFrame = [age: int, job: string ... 15 more fields]\n"}]},"apps":[],"jobName":"paragraph_1605777514754_1896149126","id":"20201119-091834_818225723","dateCreated":"2020-11-19T09:18:34+0000","dateStarted":"2020-11-22T14:27:38+0000","dateFinished":"2020-11-22T14:27:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7493"},{"title":"Zeppelin Dynamic Form","text":"%pyspark\n\nprint(\"Hello \" + z.textbox(\"name\"))","user":"admin","dateUpdated":"2021-01-11T18:35:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{"name":"WO"},"forms":{"name":{"type":"TextBox","name":"name","displayName":"name","defaultValue":"","hidden":false,"$$hashKey":"object:8414"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Hello WO\n"}]},"apps":[],"jobName":"paragraph_1605777999437_1817087050","id":"20201119-092639_1461880911","dateCreated":"2020-11-19T09:26:39+0000","dateStarted":"2021-01-11T18:35:34+0000","dateFinished":"2021-01-11T18:35:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7494"},{"title":"Задание схемы DataFrame","text":"%pyspark\n\nschema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n\n# Create our static data\ndata = [\n    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n]\n\n# Create a DataFrame using the schema defined above\nblogs_df = spark.createDataFrame(data, schema)\n# Show the DataFrame; it should reflect our table above\nblogs_df.show()\n# Print the schema used by Spark to process the DataFrame\nprint(blogs_df.printSchema())","user":"admin","dateUpdated":"2020-12-18T19:08:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o661.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:925)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:233)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:165)\norg.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:89)\norg.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\norg.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:664)\norg.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:260)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:194)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:617)\norg.apache.zeppelin.scheduler.Job.run(Job.java:188)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:99)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o661.defaultParallelism.\\n', JavaObject id=o662), <traceback object at 0x7fb5010451b8>)"}]},"apps":[],"jobName":"paragraph_1608310720108_1025141722","id":"20201218-165840_1067443582","dateCreated":"2020-12-18T16:58:40+0000","dateStarted":"2020-12-18T16:59:08+0000","dateFinished":"2020-12-18T16:59:08+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:7495"},{"text":"%pyspark\n\nspark.read.csv(\"/user/admin/sf-fire-calls.csv\").show()","user":"admin","dateUpdated":"2021-01-11T18:54:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\n|       _c0|   _c1|           _c2|             _c3|       _c4|       _c5|                 _c6|                 _c7|                 _c8| _c9|   _c10|     _c11|       _c12|_c13|            _c14|    _c15|         _c16|   _c17|         _c18|     _c19|          _c20|                _c21|                _c22|              _c23|                _c24|                _c25|         _c26|     _c27|\n+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\n|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|      UnitType|UnitSequenceInCal...|FirePreventionDis...|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|         TRUCK|                   2|                   4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|         MEDIC|                   1|                  10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|         MEDIC|                   2|                   3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|         CHIEF|                   2|                   4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n|  20110072|   T08|       2003279|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 08:03:...|  BEALE ST/FOLSOM ST|  SF|  94105|      B03|         35|2122|               3|       3|            3|  false|         null|        1|         TRUCK|                   2|                   3|                 6|Financial Distric...|(37.7886866619654...|020110072-T08|     1.75|\n|  20110125|   E33|       2003301|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:46:...|0 Block of FARALL...|  SF|  94112|      B09|         33|8324|               3|       3|            3|  false|         null|        1|        ENGINE|                   2|                   9|                11|Oceanview/Merced/...|(37.7140353531157...|020110125-E33|2.7166667|\n|  20110130|   E36|       2003304|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:58:...|600 Block of POLK ST|  SF|  94102|      B02|         03|3114|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   2|                 6|          Tenderloin|(37.7826266328595...|020110130-E36|1.7833333|\n|  20110197|   E05|       2003343|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 12:06:...|1500 Block of WEB...|  SF|  94115|      B04|         05|3513|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   4|                 5|           Japantown|(37.784958590666,...|020110197-E05|1.5166667|\n|  20110215|   E06|       2003348|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 01:08:...|DIAMOND ST/MARKET ST|  SF|  94114|      B05|         06|5415|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   5|                 8| Castro/Upper Market|(37.7618954753708...|020110215-E06|2.7666667|\n|  20110274|   M07|       2003381|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 03:31:...|2700 Block of MIS...|  SF|  94110|      B06|         11|5525|               1|       1|            2|   true|         null|        1|         MEDIC|                   1|                   6|                 9|             Mission|(37.7530339738059...|020110274-M07|2.1833334|\n|  20110275|   T15|       2003382|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 02:59:...|BRUNSWICK ST/GUTT...|  SF|  94112|      B09|         43|6218|               3|       3|            3|  false|         null|        1|         TRUCK|                   1|                   9|                11|           Excelsior|(37.7105545807996...|020110275-T15|      2.5|\n|  20110304|   E03|       2003399|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:22:...|1000 Block of SUT...|  SF|  94109|      B04|         03|1557|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   4|                 3|            Nob Hill|(37.7881263034393...|020110304-E03|2.4166667|\n|  20110308|   E14|       2003403|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:18:...|100 Block of 21ST...|  SF|  94121|      B07|         14|7173|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   7|                 1|      Outer Richmond|(37.7850084431077...|020110308-E14|     4.95|\n|  20110313|   B10|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|         CHIEF|                   6|                   6|                 9|             Mission|(37.7547064357942...|020110313-B10|1.4166666|\n|  20110313|    D3|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|         CHIEF|                   4|                   6|                 9|             Mission|(37.7547064357942...| 020110313-D3|2.5333333|\n|  20110313|   E32|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|   true|         null|        1|        ENGINE|                   8|                   6|                 9|             Mission|(37.7547064357942...|020110313-E32|1.8833333|\n|  20110315|   RC2|       2003409|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:34:...|200 Block of LAGU...|  SF|  94116|      B08|         20|8635|               3|       3|            3|   true|         null|        1|RESCUE CAPTAIN|                   2|                   8|                 7|  West of Twin Peaks|(37.7501117393668...|020110315-RC2|     5.35|\n|  20110330|   E14|       2003417|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:51:...|BALBOA ST/PARK PR...|  SF|  94118|      B07|         31|7145|               3|       3|            3|  false|         null|        1|        ENGINE|                   1|                   7|                 1|      Inner Richmond|(37.7768682293368...|020110330-E14|      2.0|\n+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------+--------------------+------------------+--------------------+--------------------+-------------+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1605779630169_-1646613942","id":"20201119-095350_1200397555","dateCreated":"2020-11-19T09:53:50+0000","dateStarted":"2021-01-11T18:54:13+0000","dateFinished":"2021-01-11T18:54:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7496"},{"title":"Create Dataset/DataFrame via SparkSession","text":"%pyspark\n\n# create DataFrame from python list. It can infer schema for you.\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\ndf1.printSchema()\ndf1.show()","user":"BD_256_glarin","dateUpdated":"2021-01-11T18:38:59+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- country: string (nullable = true)\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| andy| 20|    USA|\n|  2| jeff| 23|  China|\n|  3|james| 18|    USA|\n+---+-----+---+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605796292606_-1672359431","id":"20201119-143132_262434761","dateCreated":"2020-11-19T14:31:32+0000","dateStarted":"2020-11-22T14:27:39+0000","dateFinished":"2020-11-22T14:27:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7497"},{"title":"Create DataFrame via DataFrameReader","text":"%pyspark\nimport os\nfrom pyspark.sql.types import *\n    \nSPARK_HOME = os.getenv('SPARK_HOME')\n\n# Read data from json file\n# link for this people.json (https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json)\n# Use hdfs path if you are using hdfs\ndf1 = spark.read.json(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.json\")\ndf1.printSchema()\ndf1.show()\n\n# Read data from csv file. You can customize it via spark.read.options. E.g. In the following example, we customize the sep and header\ndf2 = spark.read.options(sep=\";\", header=True).csv(\"file://\"  + SPARK_HOME + \"/examples/src/main/resources/people.csv\")\ndf2.printSchema()\ndf2.show()\n\n# Specify schema for your csv file\nfrom pyspark.sql.types import StructType, StringType, IntegerType\n\nschema = StructType().add(\"name\", StringType(), True) \\\n    .add(\"age\", IntegerType(), True) \\\n    .add(\"job\", StringType(), True)\n    \ndf3 = spark.read.options(sep=\";\", header=True) \\\n    .schema(schema) \\\n    .csv(\"file://\" + SPARK_HOME + \"/examples/src/main/resources/people.csv\") \ndf3.printSchema()\ndf3.show()\n\n","user":"BD_256_glarin","dateUpdated":"2021-01-11T18:39:02+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\nroot\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- job: string (nullable = true)\n\n+-----+---+---------+\n| name|age|      job|\n+-----+---+---------+\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- job: string (nullable = true)\n\n+-----+---+---------+\n| name|age|      job|\n+-----+---+---------+\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n"}]},"apps":[],"jobName":"paragraph_1605796292299_-1727857589","id":"20201119-143132_2024282004","dateCreated":"2020-11-19T14:31:32+0000","dateStarted":"2020-11-22T14:27:40+0000","dateFinished":"2020-11-22T14:27:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7498"},{"title":" new column","text":"%pyspark\n\n# withColumn could be used to add new Column\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n\ndf2 = df1.withColumn(\"age2\", df1[\"age\"] + 1)\ndf2.show()\n\n# the new column could replace the existing the column if the new column name is the same as the old column\ndf3 = df1.withColumn(\"age\", df1[\"age\"] + 1)\ndf3.show()\n\n# Besides using expression to create new column, you could also use udf to create new column\n# Use F.upper instead of upper, because the builtin udf of spark may conclifct with that of python, such as max\nimport pyspark.sql.functions as F\ndf4 = df1.withColumn(\"name\", F.upper(df1[\"name\"]))\ndf4.show()","user":"BD_256_glarin","dateUpdated":"2021-01-11T18:40:00+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+---+-------+----+\n| id| name|age|country|age2|\n+---+-----+---+-------+----+\n|  1| andy| 20|    USA|  21|\n|  2| jeff| 23|  China|  24|\n|  3|james| 18|    USA|  19|\n+---+-----+---+-------+----+\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| andy| 21|    USA|\n|  2| jeff| 24|  China|\n|  3|james| 19|    USA|\n+---+-----+---+-------+\n\n+---+-----+---+-------+\n| id| name|age|country|\n+---+-----+---+-------+\n|  1| ANDY| 20|    USA|\n|  2| JEFF| 23|  China|\n|  3|JAMES| 18|    USA|\n+---+-----+---+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801563730_2144699531","id":"20201119-155923_220546233","dateCreated":"2020-11-19T15:59:23+0000","dateStarted":"2020-11-22T14:27:41+0000","dateFinished":"2020-11-22T14:27:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7499"},{"title":"remove column","text":"%pyspark\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n# drop could be used to remove Column\ndf2 = df1.drop(\"id\")\ndf2.show()\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:42+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+-------+\n| name|age|country|\n+-----+---+-------+\n| andy| 20|    USA|\n| jeff| 23|  China|\n|james| 18|    USA|\n+-----+---+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801580202_-977456675","id":"20201119-155940_271972562","dateCreated":"2020-11-19T15:59:40+0000","dateStarted":"2020-11-22T14:27:42+0000","dateFinished":"2020-11-22T14:27:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7500"},{"title":"","text":"spark.table(\"homework.bank\").show()","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n| 30|   unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n| 35|   management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n| 30|   management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n| 59|  blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n| 35|   management| single| tertiary|     no|    747|     no|  no|cellular| 23|  feb|     141|       2|  176|       3| failure| no|\n| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n| 39|   technician|married|secondary|     no|    147|    yes|  no|cellular|  6|  may|     151|       2|   -1|       0| unknown| no|\n| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n| 43|     services|married|  primary|     no|    -88|    yes| yes|cellular| 17|  apr|     313|       1|  147|       2| failure| no|\n| 39|     services|married|secondary|     no|   9374|    yes|  no| unknown| 20|  may|     273|       1|   -1|       0| unknown| no|\n| 43|       admin.|married|secondary|     no|    264|    yes|  no|cellular| 17|  apr|     113|       2|   -1|       0| unknown| no|\n| 36|   technician|married| tertiary|     no|   1109|     no|  no|cellular| 13|  aug|     328|       2|   -1|       0| unknown| no|\n| 20|      student| single|secondary|     no|    502|     no|  no|cellular| 30|  apr|     261|       1|   -1|       0| unknown|yes|\n| 31|  blue-collar|married|secondary|     no|    360|    yes| yes|cellular| 29|  jan|      89|       1|  241|       1| failure| no|\n| 40|   management|married| tertiary|     no|    194|     no| yes|cellular| 29|  aug|     189|       2|   -1|       0| unknown| no|\n| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n| 25|  blue-collar| single|  primary|     no|   -221|    yes|  no| unknown| 23|  may|     250|       1|   -1|       0| unknown| no|\n| 31|     services|married|secondary|     no|    132|     no|  no|cellular|  7|  jul|     148|       1|  152|       1|   other| no|\n+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1605801624737_-965164284","id":"20201119-160024_1543483988","dateCreated":"2020-11-19T16:00:24+0000","dateStarted":"2020-11-22T14:27:42+0000","dateFinished":"2020-11-22T14:27:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7501"},{"title":"Select Subset of Columns","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n# select can accept a list of string of the column names\ndf2 = df1.select(\"id\", \"name\")\ndf2.show()\n\n# select can also accept a list of Column. You can create column via $ or udf\nimport pyspark.sql.functions as F\ndf3 = df1.select(df1[\"id\"], F.upper(df1[\"name\"]), df1[\"age\"] + 1)\ndf3.show()\n\n\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:43+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n| id| name|\n+---+-----+\n|  1| andy|\n|  2| jeff|\n|  3|james|\n+---+-----+\n\n+---+-----------+---------+\n| id|upper(name)|(age + 1)|\n+---+-----------+---------+\n|  1|       ANDY|       21|\n|  2|       JEFF|       24|\n|  3|      JAMES|       19|\n+---+-----------+---------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801587779_-1495995504","id":"20201119-155947_1536868283","dateCreated":"2020-11-19T15:59:47+0000","dateStarted":"2020-11-22T14:27:43+0000","dateFinished":"2020-11-22T14:27:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7502"},{"title":"Filter rows","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n\n# filter accept a Column \ndf2 = df1.filter(df1[\"age\"] >= 20)\ndf2.show()\n\n# To be noticed, you need to use \"&\" instead of \"&&\" or \"AND\" \ndf3 = df1.filter((df1[\"age\"] >= 20) & (df1[\"country\"] == \"China\"))\ndf3.show()\n\n\n\n\n\n\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:43+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  1|andy| 20|    USA|\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801590075_-317220245","id":"20201119-155950_1037202214","dateCreated":"2020-11-19T15:59:50+0000","dateStarted":"2020-11-22T14:27:43+0000","dateFinished":"2020-11-22T14:27:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7503"},{"text":"","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605801689873_-80294210","id":"20201119-160129_1998798520","dateCreated":"2020-11-19T16:01:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7504"},{"title":"Create UDF","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n            .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# Create udf create python lambda\nfrom pyspark.sql.functions import udf\nudf1 = udf(lambda e: e.upper())\ndf2 = df1.select(udf1(df1[\"name\"]))\ndf2.show()\n\n# UDF could also be used in filter, in this case the return type must be Boolean\n# We can also use annotation to create udf\nfrom pyspark.sql.types import *\n@udf(returnType=BooleanType())\ndef udf2(e):\n    if e >= 20:\n        return True;\n    else:\n        return False\n\ndf3 = df1.filter(udf2(df1[\"age\"]))\ndf3.show()\n\n# UDF could also accept more than 1 argument.\nudf3 = udf(lambda e1, e2: e1 + \"_\" + e2)\ndf4 = df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\ndf4.show()\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:44+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+\n|<lambda>(name)|\n+--------------+\n|          ANDY|\n|          JEFF|\n|         JAMES|\n+--------------+\n\n+---+----+---+-------+\n| id|name|age|country|\n+---+----+---+-------+\n|  1|andy| 20|    USA|\n|  2|jeff| 23|  China|\n+---+----+---+-------+\n\n+------------+\n|name_country|\n+------------+\n|    andy_USA|\n|  jeff_China|\n|   james_USA|\n+------------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801591170_641251486","id":"20201119-155951_457057931","dateCreated":"2020-11-19T15:59:51+0000","dateStarted":"2020-11-22T14:27:44+0000","dateFinished":"2020-11-22T14:27:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7505"},{"title":"groupBy","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n\n# You can call agg function after groupBy directly, such as count/min/max/avg/sum\ndf2 = df1.groupBy(\"country\").count()\ndf2.show()\n\n# Pass a Map if you want to do multiple aggregation\ndf3 = df1.groupBy(\"country\").agg({\"age\": \"avg\", \"id\": \"count\"})\ndf3.show()\n\nimport pyspark.sql.functions as F\n# Or you can pass a list of agg function\ndf4 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.count(df1[\"id\"]).alias(\"count\"))\ndf4.show()\n\n# You can not pass Map if you want to do multiple aggregation on the same column as the key of Map should be unique. So in this case\n# you have to pass a list of agg functions\ndf5 = df1.groupBy(\"country\").agg(F.avg(df1[\"age\"]).alias(\"avg_age\"), F.max(df1[\"age\"]).alias(\"max_age\"))\ndf5.show()\n\n\n\n\n\n\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:45+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----+\n|country|count|\n+-------+-----+\n|  China|    1|\n|    USA|    2|\n+-------+-----+\n\n+-------+---------+--------+\n|country|count(id)|avg(age)|\n+-------+---------+--------+\n|  China|        1|    23.0|\n|    USA|        2|    19.0|\n+-------+---------+--------+\n\n+-------+-------+-----+\n|country|avg_age|count|\n+-------+-------+-----+\n|  China|   23.0|    1|\n|    USA|   19.0|    2|\n+-------+-------+-----+\n\n+-------+-------+-------+\n|country|avg_age|max_age|\n+-------+-------+-------+\n|  China|   23.0|     23|\n|    USA|   19.0|     20|\n+-------+-------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801591514_-1945992599","id":"20201119-155951_1376505763","dateCreated":"2020-11-19T15:59:51+0000","dateStarted":"2020-11-22T14:27:45+0000","dateFinished":"2020-11-22T14:27:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7506"},{"text":"%pyspark\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605801941491_-813509416","id":"20201119-160541_1487929652","dateCreated":"2020-11-19T16:05:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7507"},{"title":"Join on Single Field","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, 1), (2, \"jeff\", 23, 2), (3, \"james\", 18, 3)]).toDF(\"id\", \"name\", \"age\", \"c_id\")\ndf1.show()\n\ndf2 = spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\ndf2.show()\n\n# You can just specify the key name if join on the same key\ndf3 = df1.join(df2, \"c_id\")\ndf3.show()\n\n# Or you can specify the join condition expclitly in case the key is different between tables\ndf4 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"])\ndf4.show()\n\n# You can specify the join type afte the join condition, by default it is inner join\ndf5 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"], \"left_outer\")\ndf5.show()","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:52+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+---+----+\n| id| name|age|c_id|\n+---+-----+---+----+\n|  1| andy| 20|   1|\n|  2| jeff| 23|   2|\n|  3|james| 18|   3|\n+---+-----+---+----+\n\n+----+------+\n|c_id|c_name|\n+----+------+\n|   1|   USA|\n|   2| China|\n+----+------+\n\n+----+---+----+---+------+\n|c_id| id|name|age|c_name|\n+----+---+----+---+------+\n|   1|  1|andy| 20|   USA|\n|   2|  2|jeff| 23| China|\n+----+---+----+---+------+\n\n+---+----+---+----+----+------+\n| id|name|age|c_id|c_id|c_name|\n+---+----+---+----+----+------+\n|  1|andy| 20|   1|   1|   USA|\n|  2|jeff| 23|   2|   2| China|\n+---+----+---+----+----+------+\n\n+---+-----+---+----+----+------+\n| id| name|age|c_id|c_id|c_name|\n+---+-----+---+----+----+------+\n|  1| andy| 20|   1|   1|   USA|\n|  3|james| 18|   3|null|  null|\n|  2| jeff| 23|   2|   2| China|\n+---+-----+---+----+----+------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801592019_-1276834339","id":"20201119-155952_2018022643","dateCreated":"2020-11-19T15:59:52+0000","dateStarted":"2020-11-22T14:27:52+0000","dateFinished":"2020-11-22T14:27:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7508"},{"title":"Join on Multiple Fields","text":"%pyspark\n\ndf1 = spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\ndf1.show()\n\ndf2 = spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\ndf2.show()\n\n# Join on 2 fields: key_1, key_2\n\n# You can pass a list of field name if the join field names are the same in both tables\ndf3 = df1.join(df2, [\"key_1\", \"key_2\"])\ndf3.show()\n\n# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\ndf4 = df1.join(df2, (df1[\"key_1\"] == df2[\"key_1\"]) & (df1[\"key_2\"] == df2[\"key_2\"]))\ndf4.show()\n\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:56+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+-----+-----+\n| name|age|key_1|key_2|\n+-----+---+-----+-----+\n| andy| 20|    1|    1|\n| jeff| 23|    1|    2|\n|james| 12|    2|    2|\n+-----+---+-----+-----+\n\n+-----+-----+-------+\n|key_1|key_2|country|\n+-----+-----+-------+\n|    1|    1|    USA|\n|    2|    2|  China|\n+-----+-----+-------+\n\n+-----+-----+-----+---+-------+\n|key_1|key_2| name|age|country|\n+-----+-----+-----+---+-------+\n|    1|    1| andy| 20|    USA|\n|    2|    2|james| 12|  China|\n+-----+-----+-----+---+-------+\n\n+-----+---+-----+-----+-----+-----+-------+\n| name|age|key_1|key_2|key_1|key_2|country|\n+-----+---+-----+-----+-----+-----+-------+\n| andy| 20|    1|    1|    1|    1|    USA|\n|james| 12|    2|    2|    2|    2|  China|\n+-----+---+-----+-----+-----+-----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1605801592504_1717982492","id":"20201119-155952_1524810954","dateCreated":"2020-11-19T15:59:52+0000","dateStarted":"2020-11-22T14:27:56+0000","dateFinished":"2020-11-22T14:27:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7509"},{"title":"spark.sql directly","text":"%pyspark\n\ndf1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]) \\\n           .toDF(\"id\", \"name\", \"age\", \"country\")\n# call createOrReplaceTempView first if you want to query this DataFrame via sql\ndf1.createOrReplaceTempView(\"people\")\n# SparkSession.sql return DataFrame\ndf2 = spark.sql(\"select name, age from people\")\ndf2.show()\n\n# You need to register udf if you want to use it in sql\nspark.udf.register(\"udf1\", lambda e : e.upper())\ndf3 = spark.sql(\"select udf1(name), age from people\")\ndf3.show()","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:27:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+\n| name|age|\n+-----+---+\n| andy| 20|\n| jeff| 23|\n|james| 18|\n+-----+---+\n\n+----------+---+\n|udf1(name)|age|\n+----------+---+\n|      ANDY| 20|\n|      JEFF| 23|\n|     JAMES| 18|\n+----------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1605801973107_1987584435","id":"20201119-160613_1033868239","dateCreated":"2020-11-19T16:06:13+0000","dateStarted":"2020-11-22T14:27:59+0000","dateFinished":"2020-11-22T14:27:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7510"},{"text":"\n\n","user":"BD_256_glarin","dateUpdated":"2020-11-22T14:29:38+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o769.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://bigdataanalytics-head-0.novalocal:8020/user/zeppelin/bank.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o769.partitions.\\n', JavaObject id=o770), <traceback object at 0x7f8a5423e320>)"}]},"apps":[],"jobName":"paragraph_1605801592799_-1366008764","id":"20201119-155952_1798318186","dateCreated":"2020-11-19T15:59:52+0000","dateStarted":"2020-11-22T14:28:39+0000","dateFinished":"2020-11-22T14:28:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7511"},{"text":"%pyspark\n","user":"admin","dateUpdated":"2020-11-19T16:05:36+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605801936718_932280710","id":"20201119-160536_820518788","dateCreated":"2020-11-19T16:05:36+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7512"},{"text":"%pyspark\n","user":"admin","dateUpdated":"2020-11-19T16:05:37+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605801937383_750396863","id":"20201119-160537_1246577479","dateCreated":"2020-11-19T16:05:37+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7513"},{"text":"%pyspark\n","user":"admin","dateUpdated":"2020-11-19T16:05:38+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605801938063_-1788132227","id":"20201119-160538_1567138867","dateCreated":"2020-11-19T16:05:38+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7514"},{"title":"Задание схемы DataFrame","text":"%pyspark\n\nschema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n# Create our static data\ndata = [\n    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n    \"LinkedIn\"]],\n    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n    \"twitter\", \"FB\", \"LinkedIn\"]],\n    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n    [\"twitter\", \"FB\"]],\n    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n    \"twitter\", \"FB\", \"LinkedIn\"]],\n    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n    [\"twitter\", \"LinkedIn\"]]\n]\n\n# Create a DataFrame using the schema defined above\nblogs_df = spark.createDataFrame(data, schema)\n# Show the DataFrame; it should reflect our table above\nblogs_df.show()\n# Print the schema used by Spark to process the DataFrame\nprint(blogs_df.printSchema())","user":"BD_256_glarin","dateUpdated":"2020-11-22T13:52:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------+-------+-----------------+---------+-----+--------------------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- First: string (nullable = true)\n |-- Last: string (nullable = true)\n |-- Url: string (nullable = true)\n |-- Published: string (nullable = true)\n |-- Hits: integer (nullable = true)\n |-- Campaigns: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nNone\n"}]},"apps":[],"jobName":"paragraph_1605801737737_-1786859518","id":"20201119-160217_137855542","dateCreated":"2020-11-19T16:02:17+0000","dateStarted":"2020-11-22T13:52:41+0000","dateFinished":"2020-11-22T13:52:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7515"},{"text":"%pyspark\nfrom pyspark.sql.functions import *\n\n\n# Read the file into a Spark DataFrame using the CSV\n# format by inferring the schema and specifying that the\n# file contains a header, which provides column names for comma-\n# separated fields.\nmnm_df = (spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(nmn_dataset_path))\n\n# We use the DataFrame high-level APIs. Note\n# that we don't use RDDs at all. Because some of Spark's\n# functions return the same object, we can chain function calls.\n# 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n# 2. Since we want to group each state and its M&M color count,\n# we use groupBy()\n# 3. Aggregate counts of all colors and groupBy() State and Color\n# 4 orderBy() in descending order\ncount_mnm_df = (mnm_df\n    .select(\"State\", \"Color\", \"Count\")\n    .groupBy(\"State\", \"Color\")\n    .agg(count(\"Count\").alias(\"Total\"))\n    .orderBy(\"Total\", ascending=False))\n    \n    \n# Show the resulting aggregations for all the states and colors;\n# a total count of each color per state.\n# Note show() is an action, which will trigger the above\n# query to be executed.\ncount_mnm_df.show(n=60, truncate=False)\nprint(\"Total Rows = %d\" % (count_mnm_df.count()))","user":"BD_256_glarin","dateUpdated":"2020-11-22T13:52:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+------+-----+\n|State|Color |Total|\n+-----+------+-----+\n|CA   |Yellow|1807 |\n|WA   |Green |1779 |\n|OR   |Orange|1743 |\n|TX   |Green |1737 |\n|TX   |Red   |1725 |\n|CA   |Green |1723 |\n|CO   |Yellow|1721 |\n|CA   |Brown |1718 |\n|CO   |Green |1713 |\n|NV   |Orange|1712 |\n|TX   |Yellow|1703 |\n|NV   |Green |1698 |\n|AZ   |Brown |1698 |\n|WY   |Green |1695 |\n|CO   |Blue  |1695 |\n|NM   |Red   |1690 |\n|AZ   |Orange|1689 |\n|NM   |Yellow|1688 |\n|NM   |Brown |1687 |\n|UT   |Orange|1684 |\n|NM   |Green |1682 |\n|UT   |Red   |1680 |\n|AZ   |Green |1676 |\n|NV   |Yellow|1675 |\n|NV   |Blue  |1673 |\n|WA   |Red   |1671 |\n|WY   |Red   |1670 |\n|WA   |Brown |1669 |\n|NM   |Orange|1665 |\n|WY   |Blue  |1664 |\n|WA   |Yellow|1663 |\n|WA   |Orange|1658 |\n|CA   |Orange|1657 |\n|NV   |Brown |1657 |\n|CO   |Brown |1656 |\n|CA   |Red   |1656 |\n|UT   |Blue  |1655 |\n|AZ   |Yellow|1654 |\n|TX   |Orange|1652 |\n|AZ   |Red   |1648 |\n|OR   |Blue  |1646 |\n|OR   |Red   |1645 |\n|UT   |Yellow|1645 |\n|CO   |Orange|1642 |\n|TX   |Brown |1641 |\n|NM   |Blue  |1638 |\n|AZ   |Blue  |1636 |\n|OR   |Green |1634 |\n|UT   |Brown |1631 |\n|WY   |Yellow|1626 |\n|WA   |Blue  |1625 |\n|CO   |Red   |1624 |\n|OR   |Brown |1621 |\n|TX   |Blue  |1614 |\n|OR   |Yellow|1614 |\n|NV   |Red   |1610 |\n|CA   |Blue  |1603 |\n|WY   |Orange|1595 |\n|UT   |Green |1591 |\n|WY   |Brown |1532 |\n+-----+------+-----+\n\nTotal Rows = 60\n"}]},"apps":[],"jobName":"paragraph_1605803676541_-301001146","id":"20201119-163436_181428921","dateCreated":"2020-11-19T16:34:36+0000","dateStarted":"2020-11-22T13:52:50+0000","dateFinished":"2020-11-22T13:52:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7516"},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1605776150990_749149795","id":"20201119-085550_2026203117","dateCreated":"2020-11-19T08:55:50+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7517"}],"name":"/kotelnikov/lecture 1","id":"2FQRN7G62","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}