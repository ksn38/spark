{
  "metadata": {
    "name": "3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType\n\nhabrData \u003d spark.read.option(\"header\", True)\\\n.option(\"inferSchema\", True)\\\n.csv(\"/user/admin/habr_data.csv\")\\\n.withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n.cache()\n\nhabrData.printSchema()\nhabrData.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(\n    habrData\n    )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "По данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\ndf \u003d habrData\ndfWindow \u003d Window.partitionBy(\u0027author_name\u0027).orderBy(col(\u0027rating\u0027).desc())\n\ndf2 \u003d df.select(\u0027author_name\u0027, \u0027title\u0027, \u0027rating\u0027).\\\nwithColumn(\u0027row_number\u0027, row_number().over(dfWindow)).\\\nwhere(\u0027row_number \u003c 4\u0027)\n\nz.show(df2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "По данным habr_data получить топ (по встречаемости) английских слов из заголовков.\nВозможное решение: 1) выделение слов с помощью регулярных выражений, 2) разделение на массивы слов 3) explode массивовов 4) группировка с подсчетом встречаемости\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import regexp_replace, udf, explode, split\nfrom pyspark.sql.types import StringType, ArrayType\n\nhd \u003d habrData.select(\u0027title\u0027).\\\nwithColumn(\u0027title2\u0027, regexp_replace(col(\u0027title\u0027), \u0027[^a-zA-Z\\s]\u0027, \u0027 \u0027)).\\\nwithColumn(\u0027title3\u0027, regexp_replace(col(\u0027title2\u0027), \u0027\\s{2,}\u0027, \u0027 \u0027))\n\nhd \u003d hd.select(\u0027title\u0027, \u0027title3\u0027).\\\nwithColumn(\u0027title4\u0027, split(col(\u0027title3\u0027), \u0027 \u0027))\n\nhd \u003d hd.select(explode(hd.title4).alias(\u0027words\u0027)).\\\nna.drop(\u0027all\u0027).\\\ngroupBy(\u0027words\u0027).count().orderBy(col(\u0027count\u0027).desc())\n\nhd.printSchema()\n\nz.show(hd)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}