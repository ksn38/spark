{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "Нужно скопировать себе эту тетрадку и предоставить доступ к копии на чтение, запись и запуск тетрадки пользователю admin. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "val dates = (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n",
    "\n",
    "def generateCity(r: Double): String = if (r < 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n",
    "\n",
    "def generateCityUdf = udf(generateCity _)\n",
    "\n",
    "// spark.sql(\"drop table hw2.events_full\")\n",
    "\n",
    "for(i <- dates) {\n",
    "    uniformRDD(sc, 10000000L, 1)\n",
    "    .toDF(\"uid\")\n",
    "    .withColumn(\"date\", lit(i))\n",
    "    .withColumn(\"city\", generateCityUdf($\"uid\"))\n",
    "    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n",
    "    .withColumn(\"skew_key\", when($\"city\" === \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(\"hw2.events_full\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw2.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.0005)\n",
    ".repartition(2)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw2.sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.table(\"hw2.sample\")\n",
    ".limit(100)\n",
    ".coalesce(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw2.sample_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.table(\"hw2.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.003)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw2.sample_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.table(\"hw2.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.015)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw2.sample_very_big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n",
    "\n",
    "Ответить на вопросы:\n",
    " * какова структура таблиц\n",
    " * сколько в них записей \n",
    " * сколько места занимают данные\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "print(spark.table(\"hw2.events_full\").count())\n",
    "z.show(spark.table(\"hw2.events_full\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df = spark.table(\"hw2.sample\")\n",
    "print('sample', df.count())\n",
    "\n",
    "print('sample_small', spark.table(\"hw2.sample_small\").count())\n",
    "print('sample_big', spark.table(\"hw2.sample_big\").count())\n",
    "print('sample_very_big', spark.table(\"hw2.sample_very_big\").count())\n",
    "\n",
    "# z.show(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls /apps/spark/warehouse/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Получить планы запросов для джойна большой таблицы hw2.events_full с каждой из таблиц hw2.sample, hw2.sample_big, hw2.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? \n",
    "\n",
    "BroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample\"), \"event_id\")\\\n",
    ".explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample_big\"), \"event_id\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample_very_big\"), \"event_id\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\n",
    "|\n",
    "|\n",
    "BroadcastHashJoin используется в первых двух случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Выполнить джойны с таблицами  hw2.sample,  hw2.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении\n",
    "\n",
    "Зайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample\"), \"event_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample_big\"), \"event_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw2.events_full\").rdd.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw2.sample\").rdd.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw2.sample_big\").rdd.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw2.sample_very_big\").rdd.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "println(\"185.241.193.174:8088/proxy/\" + sc.applicationId + \"/jobs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\n",
    "|\n",
    "|\n",
    "63 tasks на каждый джоин: 2 на запуск экзекьютора, 61 на подсчет (60 для подсчета в каждой партиции events_full со скопированными через бродкаст малыми таблицами и 1 для общего результата). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизировать джойн с таблицами hw2.sample_big, hw2.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(broadcast(spark.table(\"hw2.sample_big\")), \"event_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(broadcast(spark.table(\"hw2.sample_very_big\")), \"event_id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\n",
    "|\n",
    "|\n",
    "Для первого запроса ничего не поменялось, так как sample_big меньше значения в автоматической бродкаст команде. Для sample_very_big не нашлось памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Отключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw2.sample, сравнить время выполнения запроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw2.events_full\")\\\n",
    ".join(spark.table(\"hw2.sample\"), \"event_id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|\n",
    "|\n",
    "|\n",
    "Broadcast Hash Join оказался в 3 раза быстрее shuffle sort-merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"26214400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"clear cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.\n",
    "\n",
    "Датафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "skew_df = spark.table(\"hw2.events_full\")\\\n",
    ".where(\"date = '2020-11-01'\")\\\n",
    ".repartition(30, col(\"city\"))\\\n",
    ".cache()\n",
    "\n",
    "skew_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.\n",
    "\n",
    "В spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют -- это и является проблемой, которую предлагается решить далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "skew_df.groupBy('city').\\\n",
    "agg(countDistinct('event_id').alias('event_count')).\\\n",
    "orderBy('event_count', ascending=False).\\\n",
    "take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "println(\"185.241.193.174:8088/proxy/\" + sc.applicationId + \"/jobs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "z.show(skew_df.groupBy('city').count().orderBy('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "z.show(skew_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "один из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num -- количество партиций, на которые будет перемешан исходный датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "skew_df.\\\n",
    "repartition(5).\\\n",
    "groupBy('city').\\\n",
    "agg(countDistinct('event_id').alias('event_count')).\\\n",
    "orderBy('event_count', ascending=False).\\\n",
    "take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "Другой способ исправить неравномерность по ключу -- создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city='BIG_CITY', которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand -- случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n",
    "\n",
    "Такая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n",
    "\n",
    "Что нужно реализовать:\n",
    "* добавить синтетический ключ\n",
    "* группировка по синтетическому ключу\n",
    "* восстановление исходного значения\n",
    "* группировка по исходной колонке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import when, rand, round, expr, lit, sum, countDistinct\n",
    "\n",
    "salt_size = 10\n",
    "\n",
    "skew_df.withColumn('salt_index', round(100 * rand())).\\\n",
    "withColumn('city_salt', when(col('city') == 'BIG_CITY', expr('CONCAT(city, salt_index)')).otherwise(col('city'))).\\\n",
    "groupBy('city_salt').\\\n",
    "agg(countDistinct('event_id').alias('count')).\\\n",
    "withColumn('city', when(expr('city_salt not like \"SMALL%\"'), lit('BIG_CITY')).otherwise(col('city_salt'))).\\\n",
    "groupBy('city').\\\n",
    "agg(sum('count').alias('count')).\\\n",
    "orderBy(col('count'), ascending=False).\\\n",
    "show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "2"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
